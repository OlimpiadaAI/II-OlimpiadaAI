{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a32ad60-78f0-45da-97f7-a5ddb6243f14",
   "metadata": {},
   "source": [
    "# Kredytobranie - Wyjaśnianie Decyzji Modelu\n",
    "\n",
    "![](https://live.staticflickr.com/65535/54368714616_e90a2c644c_z.jpg)\n",
    "\n",
    "*Obraz wygenerowany za pomocą ChatGPT.*\n",
    "\n",
    "## Wstęp\n",
    "Wyobraź sobie, że jesteś analitykiem danych w firmie zajmującej się oceną ryzyka kredytowego. \n",
    "Twój zespół opracował model, który na podstawie kluczowych wskaźników finansowych \n",
    "podejmuje decyzje o przyznaniu kredytu klientom. Model działa sprawnie, ale pojawił się \n",
    "problem - klienci, którym odmówiono kredytu, domagają się konkretnych wyjaśnień.\n",
    "\n",
    "Kierownictwo firmy zdaje sobie sprawę, że samo powiedzenie \"komputer tak zdecydował\" \n",
    "nie jest wystarczające. Potrzebują czegoś więcej - konkretnych wskazówek dla klientów, \n",
    "co mogliby zmienić w swojej sytuacji finansowej, aby otrzymać pozytywną decyzję kredytową.\n",
    "\n",
    "Jako specjalista ds. uczenia maszynowego zostajesz poproszony o opracowanie systemu, \n",
    "który pomoże zrozumieć decyzje modelu i wskaże klientom ścieżkę do uzyskania kredytu.\n",
    "\n",
    "\n",
    "## Zadanie\n",
    "Na szczęście, aby lepiej zrozumieć problem i wypracować rozwiązanie, będziesz pracował na uproszczonym,\n",
    "dwuwymiarowym zbiorze danych. Umożliwi to wizualizację wyników i lepsze zrozumienie działania\n",
    "Twojego systemu wyjaśnień.\n",
    "\n",
    "Twoim zadaniem jest zaproponowanie *metody do generowania wyjaśnień* dla odrzuconych wniosków kredytowych, która będzie proponować realistyczne zmiany w wartościach wskaźników finansowych i ostatecznie doprowadzi do pozytywnej decyzji klasyfikatora - sieci neuronowej.\n",
    "\n",
    "W trakcie pracy nad rozwiązaniem będziesz mógł zobaczyć rezultaty na wykresach, które pokażą:\n",
    "- Początkowe położenie obserwacji do wyjaśnienia;\n",
    "- Granicę decyzyjną klasyfikatora;\n",
    "- Proponowane zmiany w postaci wektorów oraz końcowych propozycji wyjaśnień;\n",
    "- Wyestymowany rozkład gęstości danych treningowych, który pomoże ocenić realność proponowanych zmian.\n",
    "\n",
    "### Dane\n",
    "Dostępne dla Ciebie w tym zadaniu dane to:\n",
    "- Zbiór danych treningowych;\n",
    "- Zbiór danych do wyjaśnienia;\n",
    "- Model dyskryminujący wytrenowany na danych treningowych; ten model będziesz wyjaśniać;\n",
    "- Model generatywny wykorzystywany do estymacji gęstości rozkładu danych treningowych.\n",
    "\n",
    "W szczególności, w swojej metodzie do generowania wyjaśnień możesz wykorzystywać jedynie model dyskryminujący, model generatywny oraz dane do wyjaśnienia. Dane treningowe służą jedynie do lepszego zobrazowania celu zadania. \n",
    "\n",
    "Twoje rozwiązanie zostanie ostatecznie przetestowane na Platformie Konkursowej na ukrytym zestawie danych testowych, który obejmuje nowe dane treningowe, dane do wyjaśnień oraz model dyskryminatywny i generatywny. Charakterystyka danych testowych nie będzie znacząco odbiegać od zestawu danych udostępnionego do zbudowania rozwiązania. Dodatkowe będą dostępne dla Ciebie dane walidacyjne na Platformie Konkursowej, na których będziesz mógł upewnić się, że całość rozwiązania wykonuje się poprawnie.\n",
    "\n",
    "### Kryterium Oceny\n",
    "Jak możesz się spodziewać, w ewaluacji będziemy oceniać trzy kluczowe aspekty Twojego rozwiązania:\n",
    "1. **Skuteczność Zmiany Decyzji Klasyfikatora** - czy Twoje propozycje faktycznie prowadzą do przyznania kredytu;\n",
    "2. **Realistyczność Wyjaśnień** - czy znajdują się one w obszarze podobnym do danych treningowych, czyli czy są osiągalne dla klientów;\n",
    "3. **Odległość Wyjaśnień** - czy proponowane modyfikacje są możliwie najmniejsze, aby nie obciążać klienta nadmiernymi zmianami w jego sytuacji finansowej.\n",
    "\n",
    "Ponieważ zależy nam na satysfakcji klientów, każdy z tych aspektów będzie musiał przekroczyć pewien próg, abyś otrzymał za niego punkty. Dodatkowo, każdy z nich będzie miał wpływ na końcową ocenę Twojego rozwiązania, zgodnie z formułami przedstawionymi poniżej, a Twój finalny wynik będzie znajdował się w przedziale $[0, 100]$.\n",
    "\n",
    "Ocena rozwiązania opiera się na trzech głównych metrykach:\n",
    "\n",
    "**Skuteczność Zmiany Decyzji Klasyfikatora ($V$)** - Miara określająca procent wygenerowanych wyjaśnień, które skutecznie zmieniają decyzję klasyfikatora:\n",
    "\n",
    "$$V = \\begin{cases}\n",
    "0, & \\text{jeśli } validity < 0.50 \\\\\n",
    "\\frac{validity - 0.50}{1.00 - 0.50}, & \\text{jeśli } 0.50 \\leq validity \\leq 1.00 \\\\\n",
    "1, & \\text{jeśli } validity > 1.00\n",
    "\\end{cases}$$\n",
    "\n",
    "gdzie *validity* jest zdefiniowane w następujący sposób:\n",
    "$$\\text{validity} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}[f(\\mathbf{{x'}_i}) > 0.5],$$\n",
    "gdzie $\\mathbf{{x'}_i}$ to zapronowane wyjaśnienie dla obserwacji $i$, $f$ to model dyskryminatywny, a $N$ to liczba obserwacji.\n",
    "\n",
    "**Realistyczność Wyjaśnień ($P$)** - Miara określająca procent wygenerowanych wyjaśnień, które są uznawane za realistyczne:\n",
    "\n",
    "$$P = \\begin{cases}\n",
    "0, & \\text{jeśli } plausibility < 0.50 \\\\\n",
    "\\frac{plausibility - 0.50}{1.00 - 0.50}, & \\text{jeśli } 0.50 \\leq plausibility \\leq 1.00 \\\\\n",
    "1, & \\text{jeśli } plausibility > 1.00\n",
    "\\end{cases}$$\n",
    "\n",
    "gdzie *plausibility* jest zdefiniowane w następujący sposób:\n",
    "$$\\text{plausibility} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}[\\log{P(\\mathbf{{x'}_i}|y')} \\geq \\text{log\\_prob\\_threshold}],$$\n",
    "gdzie $\\log{P(\\mathbf{{x'}_i}|y')}$ to logarytm prawdopodobieństwa zapronowanego $i$-tego wyjaśnienia $\\mathbf{{x'}_i}$ pod warunkiem klasy docelowej $y'$ - jest to wynik funkcji `forward` modelu generatywnego `gen_model`. Natomiast $\\text{log\\_prob\\_threshold}$ to próg logarytmu prawdopodobieństwa, który zapronowane wyjaśnienie musi przekroczyć i został on wcześniej wyznaczony na podstawie danych treningowych.\n",
    "\n",
    "**Odległość Wyjaśnień ($D$)** - Miara określająca, jak bardzo proponowane zmiany różnią się od oryginalnych danych klienta:\n",
    "\n",
    "$$D = \\begin{cases} \n",
    "1, & \\text{jeśli } \\text{odległość L2} < 0.22 \\\\\n",
    "\\frac{0.30 - \\text{odległość L2}}{0.30 - 0.22}, & \\text{jeśli } 0.22 \\leq \\text{odległość L2} \\leq 0.30 \\\\\n",
    "0, & \\text{jeśli } \\text{odległość L2} > 0.30.\n",
    "\\end{cases}$$\n",
    "\n",
    "**Ostateczna Formuła Oceny**\n",
    "Końcowa ocena jest kombinacją powyższych metryk zgodnie ze wzorem:\n",
    "\n",
    "$$S = 100 \\cdot V \\cdot \\left(\\frac{D}{2} + \\frac{P}{2}\\right)$$\n",
    "\n",
    "Ta formuła wyraża, że Skuteczność Zmiany Decyzji Klasyfikatora ($V$) jest mnożona przez Odległość Wyjaśnień ($D$) i Realistyczność Wyjaśnień ($P$). Oznacza to, że aby uzyskać dobry wynik, rozwiązanie musi być skuteczne w zmianie decyzji klasyfikatora, jednocześnie proponując zmiany, które są zarówno realistyczne, jak i efektywne (minimalne). Finalny wynik $S$ mieści się w przedziale $[0, 100]$, gdzie:\n",
    "- Wartości bliskie $0$ wskazują na słabe rozwiązanie;\n",
    "- Wartości bliskie $100$ wskazują na doskonałe rozwiązanie, które skutecznie zmienia decyzje klasyfikatora przy jednoczesnym zachowaniu realistyczności i minimalności zmian.\n",
    "\n",
    "## Ograniczenia\n",
    "- Twoje rozwiązanie będzie testowane na Platformie Konkursowej bez dostępu do internetu.\n",
    "- Ewaluacja Twojego finalnego rozwiązania na Platformie Konkursowej nie może trwać dłużej niż 2 minuty.\n",
    "- Twoje rozwiązanie nie może korzystać ze zbioru treningowego, i.e., `X_train`, `y_train`.\n",
    "- Dostępne biblioteki: Matplotlib, Numpy, Pandas, PyTorch, Scikit-Learn\n",
    "\n",
    "## Uwagi i Wskazówki\n",
    "- Warto zmienić funkcję straty, aby uzyskać lepsze wyniki.\n",
    "- Warto wykorzystać dostarczony model generatywny do estymacji gęstości rozkładu danych treningowych.\n",
    "\n",
    "## Pliki Zgłoszeniowe\n",
    "Ten notebook uzupełniony o Twoje rozwiązanie (patrz funkcja `your_generate_explanations`).\n",
    "\n",
    "## Ewaluacja\n",
    "Pamiętaj, że podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`.\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy 0 a 100 punktów. Liczba punktów, którą zdobędziesz, będzie wyliczona na (tajnym) zbiorze testowym na Platformie Konkursowej na podstawie wyżej wspomnianego wzoru, zaokrąglona do liczby całkowitej. Jeśli Twoje rozwiązanie nie będzie spełniało powyższych kryteriów lub nie będzie wykonywać się prawidłowo, otrzymasz za zadanie 0 punktów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59f10b-93e2-44b4-988e-34c7c2a2e099",
   "metadata": {},
   "source": [
    "# Kod Startowy\n",
    "W tej sekcji inicjalizujemy środowisko poprzez zaimportowanie potrzebnych bibliotek i funkcji. Przygotowany kod ułatwi Tobie efektywne operowanie na danych i budowanie właściwego rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4cbd71d21eb7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:40.233830Z",
     "start_time": "2025-01-15T17:13:40.227566Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "FINAL_EVALUATION_MODE = False  # Podczas sprawdzania ustawimy tę flagę na True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99b08f-2d9f-43ab-abc9-df026a3f5e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:44.719948Z",
     "start_time": "2025-01-15T17:13:40.250547Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "# Importy bibliotek\n",
    "import os\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ac470-dacb-4f8e-8129-cafad91b2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "# Reprodukowalność kodu\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be682410-6f2c-451d-818b-144b2d92ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "# Komórka zawierająca definicje klas modeli.\n",
    "\n",
    "## Multilayer Perceptron ##\n",
    "\n",
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_layer_sizes: List[int],\n",
    "        target_size: int,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.target_size = target_size\n",
    "        self.input_size = input_size\n",
    "        layer_sizes = [input_size] + hidden_layer_sizes + [target_size]\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(torch.nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.target_size = target_size\n",
    "        if target_size == 1:\n",
    "            self.final_activation = torch.nn.Sigmoid()\n",
    "            self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "            self.prep_for_loss = lambda x: x.view(-1, 1).float()\n",
    "        else:\n",
    "            self.final_activation = torch.nn.Softmax(dim=1)\n",
    "            self.criterion = torch.nn.CrossEntropyLoss()\n",
    "            self.prep_for_loss = lambda x: x.view(-1).long()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            if i == len(self.layers) - 1:\n",
    "                x = self.layers[i](x)\n",
    "            else:\n",
    "                x = self.relu(self.layers[i](x))\n",
    "        return x\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        test_loader: torch.utils.data.DataLoader = None,\n",
    "        epochs: int = 200,\n",
    "        lr: float = 0.001,\n",
    "        patience: int = 20,\n",
    "        eps: float = 1e-3,\n",
    "        checkpoint_path: str = \"best_model.pth\",\n",
    "    ):\n",
    "        min_test_loss = float(\"inf\")\n",
    "        optimizer = torch.optim.RAdam(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0\n",
    "            test_loss = 0\n",
    "            for i, (examples, labels) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(examples)\n",
    "                loss = self.criterion(outputs, self.prep_for_loss(labels))\n",
    "                train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss /= len(train_loader)\n",
    "            if test_loader:\n",
    "                with torch.no_grad():\n",
    "                    for i, (examples, labels) in enumerate(test_loader):\n",
    "                        outputs = self.forward(examples)\n",
    "                        loss = self.criterion(outputs, self.prep_for_loss(labels))\n",
    "                        test_loss += loss.item()\n",
    "                        # Early stopping\n",
    "                    test_loss /= len(test_loader)\n",
    "                if test_loss < (min_test_loss - eps):\n",
    "                    min_test_loss = test_loss\n",
    "                    patience_counter = 0\n",
    "                    self.save(checkpoint_path)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                if patience_counter > patience:\n",
    "                    break\n",
    "                self.load(checkpoint_path)\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Train: {train_loss:.4f}, test: {test_loss:.4f}, patience: {patience_counter}\"\n",
    "            )\n",
    "\n",
    "    def predict(self, X_test: Union[np.ndarray, torch.Tensor]):\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float()\n",
    "        with torch.no_grad():\n",
    "            probs = self.predict_proba(X_test) > 0.5\n",
    "            return probs.squeeze().float()\n",
    "\n",
    "    def predict_proba(self, X_test: Union[np.ndarray, torch.Tensor]):\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float()\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(X_test)\n",
    "            probs = self.final_activation(logits)\n",
    "            return probs.float()\n",
    "\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "## Kernel Density Estimator ##\n",
    "\n",
    "class GaussianKernel(nn.Module):\n",
    "    \"\"\"Implementation of the Gaussian kernel.\"\"\"\n",
    "    \n",
    "    def __init__(self, bandwidth=1.0):\n",
    "        \"\"\"Initializes a new Kernel.\n",
    "\n",
    "        Args:\n",
    "            bandwidth: The kernel's (band)width.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "    def _diffs(self, test_Xs, train_Xs):\n",
    "        \"\"\"Computes difference between each x in test_Xs with all train_Xs.\"\"\"\n",
    "        test_Xs = test_Xs.view(test_Xs.shape[0], 1, *test_Xs.shape[1:])\n",
    "        train_Xs = train_Xs.view(1, train_Xs.shape[0], *train_Xs.shape[1:])\n",
    "        return test_Xs - train_Xs\n",
    "\n",
    "    def forward(self, test_Xs, train_Xs):\n",
    "        \"\"\"Computes log p(x) for each x in test_Xs given train_Xs.\"\"\"\n",
    "        n, d = train_Xs.shape\n",
    "        n, h = torch.tensor(n, dtype=torch.float32), torch.tensor(self.bandwidth)\n",
    "        pi = torch.tensor(np.pi)\n",
    "\n",
    "        Z = 0.5 * d * torch.log(2 * pi) + d * torch.log(h) + torch.log(n)\n",
    "        diffs = self._diffs(test_Xs, train_Xs) / h\n",
    "        log_exp = -0.5 * torch.norm(diffs, p=2, dim=-1) ** 2\n",
    "\n",
    "        return torch.logsumexp(log_exp - Z, dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, train_Xs):\n",
    "        \"\"\"Generates samples from the kernel distribution.\"\"\"\n",
    "        device = train_Xs.device\n",
    "        noise = torch.randn(train_Xs.shape, device=device) * self.bandwidth\n",
    "        return train_Xs + noise\n",
    "\n",
    "\n",
    "class KernelDensityEstimator(nn.Module):\n",
    "    \"\"\"The KernelDensityEstimator model.\"\"\"\n",
    "\n",
    "    def __init__(self, train_Xs, kernel=None):\n",
    "        \"\"\"Initializes a new KernelDensityEstimator.\n",
    "\n",
    "        Args:\n",
    "            train_Xs: The \"training\" data to use when estimating probabilities.\n",
    "            kernel: The kernel to place on each of the train_Xs.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel = kernel or GaussianKernel()\n",
    "        self.train_Xs = nn.Parameter(train_Xs, requires_grad=False)\n",
    "        assert len(self.train_Xs.shape) == 2, \"Input cannot have more than two axes.\"\n",
    "\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        \"\"\"Saves input tensor attributes so they can be accessed during sampling.\"\"\"\n",
    "        if getattr(self, \"_c\", None) is None and x.dim() == 4:\n",
    "            _, c, h, w = x.shape\n",
    "            self._create_shape_buffers(c, h, w)\n",
    "        return super().__call__(x, *args, **kwargs)\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        \"\"\"Registers dynamic buffers before loading the model state.\"\"\"\n",
    "        if \"_c\" in state_dict and not getattr(self, \"_c\", None):\n",
    "            c, h, w = state_dict[\"_c\"], state_dict[\"_h\"], state_dict[\"_w\"]\n",
    "            self._create_shape_buffers(c, h, w)\n",
    "        super().load_state_dict(state_dict, strict)\n",
    "\n",
    "    def _create_shape_buffers(self, channels, height, width):\n",
    "        channels = channels if torch.is_tensor(channels) else torch.tensor(channels)\n",
    "        height = height if torch.is_tensor(height) else torch.tensor(height)\n",
    "        width = width if torch.is_tensor(width) else torch.tensor(width)\n",
    "        self.register_buffer(\"_c\", channels)\n",
    "        self.register_buffer(\"_h\", height)\n",
    "        self.register_buffer(\"_w\", width)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.train_Xs.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.kernel(x, self.train_Xs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples):\n",
    "        idxs = np.random.choice(range(len(self.train_Xs)), size=n_samples)\n",
    "        return self.kernel.sample(self.train_Xs[idxs])\n",
    "\n",
    "\n",
    "class KDE(torch.nn.Module):\n",
    "    def __init__(self, bandwidth=0.1, **kwargs):  # Ignores kwargs!\n",
    "        super(KDE, self).__init__()\n",
    "        self.bandwidth = bandwidth\n",
    "        self.models = nn.ModuleDict()\n",
    "\n",
    "    def _context_to_key(self, context):\n",
    "        return str(int(context))\n",
    "\n",
    "    def _get_model_for_context(self, context):\n",
    "        key = self._context_to_key(context)\n",
    "        if key not in self.models:\n",
    "            raise ValueError(f\"Context {key} not found in the model.\")\n",
    "        return self.models[key]\n",
    "\n",
    "    def load_state_dict(\n",
    "            self,\n",
    "            state_dict,\n",
    "            strict: bool = True,\n",
    "            assign: bool = False,\n",
    "    ):\n",
    "        for key in state_dict.keys():\n",
    "            if key.startswith(\"models.\"):\n",
    "                self.models[key.split(\".\")[1]] = KernelDensityEstimator(\n",
    "                    state_dict[key], kernel=GaussianKernel(bandwidth=self.bandwidth)\n",
    "                )\n",
    "        return super().load_state_dict(state_dict, strict, assign)\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            train_loader: torch.utils.data.DataLoader,\n",
    "            test_loader: torch.utils.data.DataLoader,\n",
    "            checkpoint_path: str = \"best_model.pth\",\n",
    "            **kwargs,\n",
    "    ):\n",
    "        train_Xs, train_ys = train_loader.dataset.tensors\n",
    "        train_ys = train_ys.view(-1)\n",
    "        for y in train_ys.unique():\n",
    "            idxs = train_ys == y\n",
    "            self.models.update(\n",
    "                {\n",
    "                    self._context_to_key(y.item()): KernelDensityEstimator(\n",
    "                        train_Xs[idxs], kernel=GaussianKernel(bandwidth=self.bandwidth)\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        self.save(checkpoint_path)\n",
    "\n",
    "        train_log_probs = self.predict_log_prob(train_loader)\n",
    "        test_log_probs = self.predict_log_prob(test_loader)\n",
    "        print(f\"Train log-likelihood: {train_log_probs.float().mean()}\")\n",
    "        print(f\"Test log-likelihood: {test_log_probs.float().mean()}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, context: torch.Tensor):\n",
    "        preds = torch.zeros_like(context)\n",
    "        for i in range(x.shape[0]):\n",
    "            model = self._get_model_for_context(context[i].item())\n",
    "            preds[i] = model(x[i].unsqueeze(0))\n",
    "        return preds.view(-1)\n",
    "\n",
    "    def predict_log_prob(self, dataloader: torch.utils.data.DataLoader):\n",
    "        inputs, context = dataloader.dataset.tensors\n",
    "        preds = self(inputs, context)\n",
    "        preds = torch.zeros_like(context, dtype=torch.float32)\n",
    "        for i in range(inputs.shape[0]):\n",
    "            model = self._get_model_for_context(context[i].item())\n",
    "            preds[i] = model(inputs[i].unsqueeze(0))\n",
    "        return preds\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44051f-d5b6-4ec9-971a-0a9605a3d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "# Komórka zawierająca funkcje pomocnicze do rysowania wykresów.\n",
    "\n",
    "## FUNKCJE POMOCNICZE ##\n",
    "\n",
    "def _plot_generative_model_distribution(ax, model, log_prob_threshold=None):\n",
    "    xline = torch.linspace(-0, 1, 200)\n",
    "    yline = torch.linspace(-0, 1, 200)\n",
    "    xgrid, ygrid = torch.meshgrid(xline, yline)\n",
    "    xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        zgrid = model(xyinput, torch.ones(40000, 1)).exp().reshape(200, 200)\n",
    "        zgrid = zgrid.numpy()\n",
    "        _ = ax.contour(\n",
    "            xgrid.numpy(),\n",
    "            ygrid.numpy(),\n",
    "            zgrid,\n",
    "            levels=10,\n",
    "            cmap=\"Greys\",\n",
    "            linewidths=0.4,\n",
    "            antialiased=True,\n",
    "        )\n",
    "        ax.plot([], [], color='grey', alpha=0.3, label=\"Poziomice gęstości danych treningowych\")\n",
    "\n",
    "    if log_prob_threshold is not None:\n",
    "        prob_threshold_exp = np.exp(log_prob_threshold)\n",
    "        _ = ax.contourf(\n",
    "            xgrid.numpy(),\n",
    "            ygrid.numpy(),\n",
    "            zgrid,\n",
    "            levels=[prob_threshold_exp, prob_threshold_exp * 10.00],\n",
    "            alpha=0.1,\n",
    "            colors=\"#DC143C\",\n",
    "        )  # 10.00 is an arbitrary huge value to colour the whole distribution.\n",
    "        ax.plot([], [], color='#DC143C', alpha=0.3, label=\"Obszar realistyczności wyjaśnień\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_classifier_decision_region(ax, model):\n",
    "    xline = torch.linspace(0, 1, 1000)\n",
    "    yline = torch.linspace(0, 1, 1000)\n",
    "    xgrid, ygrid = torch.meshgrid(xline, yline, indexing=\"ij\")\n",
    "    xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "    y_hat = model.predict(xyinput)\n",
    "    y_hat = y_hat.reshape(1000, 1000)\n",
    "\n",
    "    display = DecisionBoundaryDisplay(xx0=xgrid, xx1=ygrid, response=y_hat)\n",
    "    display.plot(plot_method=\"contour\", ax=ax, alpha=0.3)\n",
    "    ax.plot([], [], color='green', alpha=0.3, label=\"Granica decyzyjna modelu\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_propositions(ax, propositions):\n",
    "    ax.scatter(\n",
    "        propositions[:, 0], \n",
    "        propositions[:, 1], \n",
    "        c=\"orange\", \n",
    "        s=50, \n",
    "        alpha=0.8,\n",
    "        label=\"Wyjaśnienia\"\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_observations(ax, observations, targets):\n",
    "    indices = targets == 0\n",
    "\n",
    "    observations_0 = observations[indices]\n",
    "    observations_1 = observations[~indices]\n",
    "    \n",
    "    ax.scatter(\n",
    "        observations_0[:, 0],\n",
    "        observations_0[:, 1],\n",
    "        c=\"blue\",\n",
    "        s=50,\n",
    "        alpha=0.8,\n",
    "        label=\"Obserwacje z klasy 0\"\n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        observations_1[:, 0],\n",
    "        observations_1[:, 1],\n",
    "        c=\"red\",\n",
    "        s=50,\n",
    "        alpha=0.8,\n",
    "        label=\"Obserwacje z klasy 1\"\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_observations_to_explain(ax, observations):\n",
    "    ax.scatter(\n",
    "        observations[:, 0],\n",
    "        observations[:, 1],\n",
    "        c=\"blue\",\n",
    "        s=50,\n",
    "        alpha=0.8,\n",
    "        label=\"Obserwacje do wyjaśnienia\"\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_arrows(ax, observations, propositions):\n",
    "    for i in range(len(observations)):\n",
    "        ax.arrow(\n",
    "            observations[i, 0],\n",
    "            observations[i, 1],\n",
    "            propositions[i, 0] - observations[i, 0],\n",
    "            propositions[i, 1] - observations[i, 1],\n",
    "            width=0.001,\n",
    "            lw=0.001,\n",
    "            length_includes_head=True,\n",
    "            alpha=0.5,\n",
    "            color=\"k\",\n",
    "        )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_initial_setup(X_orig, y_orig, disc_model=None, gen_model=None, log_prob_threshold=None):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(20, 12)\n",
    "\n",
    "    ax = _plot_observations(ax, X_orig, y_orig)\n",
    "\n",
    "    if disc_model:\n",
    "        ax = _plot_classifier_decision_region(ax, disc_model)\n",
    "    if gen_model:\n",
    "        ax = _plot_generative_model_distribution(\n",
    "            ax, gen_model, log_prob_threshold=log_prob_threshold\n",
    "        )\n",
    "\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_explanation_setup(X_orig, X_new=None, disc_model=None, gen_model=None, log_prob_threshold=None):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(20, 12)\n",
    "\n",
    "    ax = _plot_observations_to_explain(ax, X_orig)\n",
    "    \n",
    "    if disc_model:\n",
    "        ax = _plot_classifier_decision_region(ax, disc_model)\n",
    "    \n",
    "    if gen_model:\n",
    "        ax = _plot_generative_model_distribution(\n",
    "            ax, gen_model, log_prob_threshold=log_prob_threshold\n",
    "        )\n",
    "    \n",
    "    if X_new is not None:\n",
    "        assert (\n",
    "                X_orig.shape == X_new.shape\n",
    "        ), f\"Sizes of test set and counterfactuals are not equal. Actual sizes: X_orig: {X_orig.shape}, X_cf: {X_new.shape}\"\n",
    "\n",
    "        ax = _plot_propositions(ax, X_new)\n",
    "        ax = _plot_arrows(ax, X_orig, X_new)\n",
    "    \n",
    "    ax.legend(loc=\"lower left\")\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141b9a8-e88a-48aa-b58b-29c8a4aa0839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:44.841754Z",
     "start_time": "2025-01-15T17:13:44.838710Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Definicja ścieżek do zbiorów danych oraz modeli\n",
    "DATA_DIRECTORY = 'data'\n",
    "\n",
    "ARRAY_X_TRAIN_PATH = f'{DATA_DIRECTORY}/x_train.npy'\n",
    "ARRAY_X_EXPLAIN_PATH = f'{DATA_DIRECTORY}/x_explain.npy'\n",
    "ARRAY_Y_TRAIN_PATH = f'{DATA_DIRECTORY}/y_train.npy'\n",
    "\n",
    "DISC_MODEL_PATH = f'{DATA_DIRECTORY}/disc_model.pth'\n",
    "GEN_MODEL_PATH = f'{DATA_DIRECTORY}/gen_model.pth'\n",
    "LOG_PROB_THRESHOLD_PATH = f'{DATA_DIRECTORY}/log_prob_threshold.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32f2e4-87b0-4fb2-9bdb-0f1959486fe5",
   "metadata": {},
   "source": [
    "## Ładowanie Danych\n",
    "W tej części zadania załadujemy dane treningowe, które zostały wykorzystane do treningu modelu dyskryminującego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b2142-3348-4a74-b883-b9e069fd3a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:44.854765Z",
     "start_time": "2025-01-15T17:13:44.851243Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "X_train = np.load(ARRAY_X_TRAIN_PATH)\n",
    "y_train = np.load(ARRAY_Y_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc66080-d606-49ca-b821-0e4ff164c7ff",
   "metadata": {},
   "source": [
    "Wyświetlmy dane treningowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4fedc-40f8-424e-9d84-2652945c46b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:45.142836Z",
     "start_time": "2025-01-15T17:13:44.898008Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_initial_setup(X_train, y_train)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a7cfb-b4dd-400d-bc2c-886397b04602",
   "metadata": {},
   "source": [
    "## Ładowanie Modelu Dyskryminujacęgo\n",
    "\n",
    "W tym zadaniu będziemy wyjaśniać model prostej sieci neuronowej, która została wcześniej wytrenowana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be05ee-93e5-4960-a56f-8cadce32e893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:45.170317Z",
     "start_time": "2025-01-15T17:13:45.160548Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "disc_model = MultilayerPerceptron(\n",
    "    input_size=2, \n",
    "    hidden_layer_sizes=[256, 256], \n",
    "    target_size=1, \n",
    "    dropout=0.1\n",
    ")\n",
    "disc_model.load(DISC_MODEL_PATH)\n",
    "disc_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d9e81-4508-46b7-aa8a-a00602bf7d1f",
   "metadata": {},
   "source": [
    "Wyświetlmy zbiór danych oraz granice decyzjną modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc529a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:47.513973Z",
     "start_time": "2025-01-15T17:13:45.188160Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_initial_setup(X_train, y_train, disc_model=disc_model)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62dac6-0495-4cfb-a3e6-0873a586c28b",
   "metadata": {},
   "source": [
    "## Realistyczność Wyjaśnień\n",
    "\n",
    "W tym zadaniu skupimy się na ważnym aspekcie generowania wyjaśnień - chcemy, aby wygenerowane punkty były realistyczne, a w naszym przypadku będziemy to defniować jako pochodzenie z obszaru o wysokiej gęstości rozkładu danych treningowych.\n",
    "   \n",
    "Zacznijmy od zapoznania się z zagadnieniem estymacji gęstości rozkładu danych. Estymacja gęstości rozkładu (density estimation) to zadanie polegające na znalezieniu funkcji $p(x)$,\n",
    "która przybliża prawdziwy rozkład prawdopodobieństwa danych $p^*(x)$. Formalnie, mając zbiór próbek\n",
    "${x_1, ..., x_n}$ pochodzących z nieznanego rozkładu $p^*(x)$, chcemy znaleźć model $p(x)$, który\n",
    "najlepiej przybliża ten rozkład.\n",
    "\n",
    "W tym zadaniu wykorzystamy model estymatora jądrowego (ang. Kernel Density Estimation (KDE)), który jest jednym z najpopularniejszych modeli estymacji gęstości rozkładu. Jako kryterium progu akceptowalności realistyczności przyjmiemy jako medianę wartości funkcji gęstości dla punktów treningowych, którą wcześniej dla Ciebie została policzona. Oznacza to, że funkcja gęstości KDE dla propozycji nowych zmiennych dla klienta powinna mieć wartość powyżej progu akceptowalności. Ten koncept jest zwizualizowany na kolejnym wykresie w postaci czerwonego obszaru. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf4683-5abd-4d2e-ba97-e4e40060e8dc",
   "metadata": {},
   "source": [
    "## Ładowanie Modelu Generatywnego wraz z Progiem Akceptowalności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22df8b-d1af-477c-8577-871042fce88c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:47.538420Z",
     "start_time": "2025-01-15T17:13:47.531569Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "gen_model = KDE(bandwidth=0.05)\n",
    "gen_model.load(GEN_MODEL_PATH)\n",
    "gen_model.eval()\n",
    "\n",
    "with open(LOG_PROB_THRESHOLD_PATH, 'r') as f:\n",
    "    log_prob_threshold = float(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533807c-c56b-4896-8919-f3a6fbb655f7",
   "metadata": {},
   "source": [
    "Wyświetlmy setup modelu, danych oraz gęstości rozkładu danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81187b-4c17-4c9d-b934-57def5c4c2de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:55.467778Z",
     "start_time": "2025-01-15T17:13:47.553990Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_initial_setup(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        disc_model=disc_model,\n",
    "        gen_model=gen_model,\n",
    "        log_prob_threshold=log_prob_threshold\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540faae4",
   "metadata": {},
   "source": [
    "## Ładowanie Danych do Wyjaśnienia\n",
    "W tej części zadania załadujemy zbiór danych do wyjaśnienia. Twoim zdaniem będzie wygenerowanie wyjaśnień dla punktów z tego zbioru danych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3fc17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:13:55.486053Z",
     "start_time": "2025-01-15T17:13:55.481679Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "X_explain = np.load(ARRAY_X_EXPLAIN_PATH)\n",
    "y_explain = np.ones((X_explain.shape[0], 1))  # Wektor 1 - klasa na którą chcemy zmienić decyzję modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2961ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:14:03.745096Z",
     "start_time": "2025-01-15T17:13:55.500196Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_explanation_setup(X_explain, disc_model=disc_model, gen_model=gen_model, log_prob_threshold=log_prob_threshold)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d9d9d",
   "metadata": {},
   "source": [
    "## Przykładowe Rozwiazanie\n",
    "Poniżej przedstawiamy uproszczone rozwiązanie, które służy jako przykład demonstrujący podstawową funkcjonalność notatnika. Może ono posłużyć jako punkt wyjścia do opracowania Twojego rozwiązania.\n",
    "\n",
    "Jednym ze sposobów rozwiązania powyższego problemu jest metoda optymalizacji punktu docelowego $x^*$ poprzez minimalizację następującej funkcji celu:\n",
    "\n",
    "$$ L(x^*) = \\text{BCE}(f(x^*), y^*) + \\lambda \\cdot |x^* - x|^2_2 $$\n",
    "\n",
    "gdzie:\n",
    "- $\\text{BCE}$ to funkcja straty binary cross-entropy\n",
    "- $f(x^*)$ to predykcja modelu dla punktu $x^*$\n",
    "- $y^*$ to pożądana klasa docelowa \n",
    "- $|x^* - x|_2^2$ to kwadrat odległości euklidesowej między punktem x* a punktem wyjściowym x\n",
    "- $\\lambda$ to parametr regulacji kompromisu między składowymi funkcji straty (w implementacji $\\lambda$=0.1)\n",
    "\n",
    "Jest to podstawowe podejście, które nie uwzględnia rozkładu danych treningowych. Poniżej znajdziesz przykładowa implementacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee461667a569ba62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:14:08.102411Z",
     "start_time": "2025-01-15T17:14:03.757010Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "def example_generate_explanations(\n",
    "        X_explain: np.ndarray, \n",
    "        y_explain: np.ndarray, \n",
    "        disc_model: MultilayerPerceptron,\n",
    "        gen_model: KDE,\n",
    "        log_prob_threshold: float,\n",
    "        verbose: bool = False\n",
    "    ) -> np.ndarray:\n",
    "    \n",
    "    num_steps: int = 1000\n",
    "    lr: float = 0.01\n",
    "\n",
    "    x_orig = torch.tensor(X_explain, dtype=torch.float32)\n",
    "    target = torch.tensor(y_explain, dtype=torch.float32)\n",
    "\n",
    "    # Definicja nowych punktów X jako wyjaśnienia.\n",
    "    x_new = torch.tensor(X_explain, requires_grad=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([x_new], lr=lr)\n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prediction Loss (Binary Cross Entropy)\n",
    "        pred = disc_model(x_new)\n",
    "        pred_loss = bce_loss(pred, target)\n",
    "\n",
    "        # Distance Loss (Squared L2)\n",
    "        dist_loss = torch.sum((x_orig - x_new)**2, axis=1, keepdim=True)\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = pred_loss + 0.1 * dist_loss\n",
    "        total_loss = total_loss.mean()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Total loss: {total_loss:.4f}\")\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    return x_new.detach().numpy()\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    X_new = example_generate_explanations(X_explain, y_explain, disc_model=disc_model, gen_model=gen_model, log_prob_threshold=log_prob_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766f47bac033d00",
   "metadata": {},
   "source": [
    "## Wizualizacja Wyjaśnień"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8acecc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:14:16.083237Z",
     "start_time": "2025-01-15T17:14:08.119502Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Wizualizacja wygenerowanych wyjaśnień\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_explanation_setup(\n",
    "        X_explain,\n",
    "        X_new=X_new,\n",
    "        disc_model=disc_model,\n",
    "        gen_model=gen_model,\n",
    "        log_prob_threshold=log_prob_threshold\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90b044-deaa-429d-8179-0227564ef5e8",
   "metadata": {},
   "source": [
    "# Twoje Rozwiązanie\n",
    "W tej sekcji należy umieścić Twoje rozwiązanie. Wprowadzaj zmiany wyłącznie tutaj!\n",
    "\n",
    "Twoim zadaniem jest implementacja funkcji ```your_generate_explanations```.\n",
    "Pamiętaj, że definicja funkcji nie powinna być zmieniana, a także wynikowa tablica rezultatów powinna być tego samego rozmiaru co tablica wejściowa punktów do wyjaśnień."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02874519-fbde-4e49-bfc7-c6eaa0796fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_generate_explanations(\n",
    "        X_explain: np.ndarray, \n",
    "        y_explain: np.ndarray, \n",
    "        disc_model: MultilayerPerceptron,\n",
    "        gen_model: KDE,\n",
    "        log_prob_threshold: float\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "    results = X_explain\n",
    "\n",
    "    assert results.shape == X_explain.shape\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f4fea9561ff15",
   "metadata": {},
   "source": [
    "# Ewaluacja\n",
    "\n",
    "Uruchomienie poniższej komórki pozwoli sprawdzić, ile punktów zdobyłoby Twoje rozwiązanie na dostępnych danych. Przed wysłaniem upewnij się, że cały notebook wykonuje się od początku do końca bez błędów i bez konieczności ingerencji użytkownika po wybraniu opcji \"Run All\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3040b-5123-4de2-a093-b9964812f983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T17:14:16.144281Z",
     "start_time": "2025-01-15T17:14:16.110423Z"
    }
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "def scale(x, lower=0.50, upper=1.00, max_points=1.0):\n",
    "    \"\"\"Funkcja skalująca liniowo wynik.\"\"\"\n",
    "    scaled = min(max(x, lower), upper)\n",
    "    return (scaled - lower) / (upper - lower) * max_points\n",
    "\n",
    "\n",
    "def calculate_average_distance(X_orig, X_new):\n",
    "    \"\"\"Oblicz średnią odległość L2 pomiędzy punktami.\"\"\"\n",
    "    distances = np.sqrt(np.sum((X_orig - X_new)**2, axis=1))\n",
    "    return np.mean(distances)\n",
    "\n",
    "\n",
    "def calculate_validity_criterion(disc_model, X_new):\n",
    "    \"\"\"Oblicz procent obserwacji, które poprawnie przekraczają granicę decyzyjną.\"\"\"    \n",
    "    with torch.no_grad():\n",
    "        cf_preds = disc_model.predict(X_new)\n",
    "    return np.mean((cf_preds > 0.5).numpy())\n",
    "\n",
    "\n",
    "def calculate_plausibility_criterion(gen_model, X_new, log_prob_threshold):\n",
    "    \"\"\"Oblicz procent obserwacji powyżej progu realistyczności.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        cf_log_probs = gen_model(torch.tensor(X_new, dtype=torch.float32), torch.ones((X_new.shape[0], 1)))\n",
    "        return torch.mean((cf_log_probs >= log_prob_threshold).float()).item()\n",
    "\n",
    "\n",
    "def calculate_final_metric(X_explain, X_new, disc_model, gen_model, verbose=True):\n",
    "    \"\"\"Oblicz ostateczną metrykę.\"\"\"\n",
    "    LOG_PROB_THRESHOLD = log_prob_threshold\n",
    "    DISTANCE_UPPER_BOUND = 0.30\n",
    "    DISTANCE_LOWER_BOUND = 0.22\n",
    "    VALIDITY_UPPER_BOUND = 1.00\n",
    "    VALIDITY_LOWER_BOUND = 0.50\n",
    "    PLAUSIBILITY_UPPER_BOUND = 1.00\n",
    "    PLAUSIBILITY_LOWER_BOUND = 0.50\n",
    "    \n",
    "    avg_distance = calculate_average_distance(X_explain, X_new)\n",
    "    distances = 1 if avg_distance < DISTANCE_LOWER_BOUND else max(0, min(1, (DISTANCE_UPPER_BOUND - avg_distance) / (DISTANCE_UPPER_BOUND - DISTANCE_LOWER_BOUND)))\n",
    "\n",
    "    validity_rate = calculate_validity_criterion(disc_model, X_new)\n",
    "    validity_rate = scale(validity_rate, VALIDITY_LOWER_BOUND, VALIDITY_UPPER_BOUND)\n",
    "    \n",
    "    plausibility_rate = calculate_plausibility_criterion(gen_model, X_new, LOG_PROB_THRESHOLD)\n",
    "    plausibility_rate = scale(plausibility_rate, PLAUSIBILITY_LOWER_BOUND, PLAUSIBILITY_UPPER_BOUND)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Średnia odległość: {avg_distance:.4f}\\n\")\n",
    "        print(f\"Wynik: Odległość wyjaśnień: {distances:.4f}\")\n",
    "        print(f\"Wynik: Skuteczność zmiany decyzji klasyfikatora: {validity_rate:.4f}\")\n",
    "        print(f\"Wynik: Skuteczność realistyczności wyjaśnień: {plausibility_rate:.4f}\")\n",
    "        print(\"-\"*30)\n",
    "    \n",
    "    # Obliczanie finalnej metryki\n",
    "    score = 100 * validity_rate * (plausibility_rate + distances) / 2\n",
    "    final_metric = int(round(score))\n",
    "    print(f\"Estymowana liczba punktów za zadanie: {final_metric}\")\n",
    "    return final_metric\n",
    "\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    X_new = your_generate_explanations(X_explain, y_explain, disc_model=disc_model, gen_model=gen_model, log_prob_threshold=log_prob_threshold)\n",
    "    final_score = calculate_final_metric(X_explain, X_new, disc_model, gen_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba4e59-ca60-4550-9fd7-6c18328aa599",
   "metadata": {},
   "source": [
    "Podczas sprawdzania model zostanie zapisany jako `your_model.pkl` i oceniony na zbiorze walidacyjnym oraz testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947eaaa-ab2a-41e6-9508-814f66998fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if FINAL_EVALUATION_MODE:\n",
    "    import cloudpickle\n",
    "\n",
    "    OUTPUT_PATH = \"file_output\"\n",
    "    FUNCTION_FILENAME = \"your_model.pkl\"\n",
    "    FUNCTION_OUTPUT_PATH = os.path.join(OUTPUT_PATH, FUNCTION_FILENAME)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "    with open(FUNCTION_OUTPUT_PATH, \"wb\") as f:\n",
    "        cloudpickle.dump(your_generate_explanations, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
