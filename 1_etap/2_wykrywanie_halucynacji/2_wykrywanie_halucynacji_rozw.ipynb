{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykrywanie Halucynacji\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/54208132682_73767c3560_b.jpg\" alt=\"Embedded Photo\" width=\"500\">\n",
    "\n",
    "*Obraz wygenerowany przy użyciu modelu DALL-E.*\n",
    "\n",
    "## Wstęp\n",
    "\n",
    "Modele językowe pomagają nam w codziennych zadaniach, takich jak poprawianie tekstów, pisanie kodu czy odpowiadanie na pytania. \n",
    "Są one również coraz częściej wykorzystywane w takich dziedzinach jak medycyna czy edukacja.\n",
    "\n",
    "Jednak skąd możemy wiedzieć, czy wygenerowane przez nie odpowiedzi są poprawne? Modele językowe nie zawsze posiadają pełną wiedzę na zadany temat, a mimo to mogą formułować odpowiedzi, które brzmią wiarygodnie, lecz w rzeczywistości wprowadzają w błąd. Takie niepoprawne odpowiedzi nazywamy halucynacjami.\n",
    "\n",
    "## Zadanie\n",
    "\n",
    "W tym zadaniu zmierzysz się z wykrywaniem halucynacji w odpowiedziach na pytania faktograficzne generowane przez duże modele językowe (LLM).\n",
    "Przeanalizujesz zbiór danych, który pomoże w ocenie, czy odpowiedzi generowane przez model językowy są faktycznie poprawne, czy zawierają halucynacje.\n",
    "\n",
    "Każdy przykład w zbiorze danych zawiera:\n",
    "\n",
    "- **Pytanie** np. \"Jaka jest główna odpowiedzialność Departamentu Obrony USA?\"\n",
    "- **Odpowiedź modelu językowego** np. \"Główną odpowiedzialnością jest obrona kraju.\"\n",
    "- **Tokeny** związane z generacją odpowiedzi.\n",
    "- **Cztery alternatywne odpowiedzi** wygenerowane z przez ten sam model z większą temperaturą.\n",
    "- **Tokeny alternatywnych odpowiedzi** wygenerowane z przez ten sam model z większą temperaturą.\n",
    "- **Prawdopodobieństwa alternatywnych odpowiedzi** wygenerowane z przez ten sam model z większą temperaturą.\n",
    "- **Etykietę (`is_correct`)** wskazującą, czy główna odpowiedź jest poprawna według zaufanego źródła.\n",
    "\n",
    "\n",
    "Przykład:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"question_id\": 34,\n",
    "        \"question\": \"What is the name of the low-cost carrier that operates as a wholly owned subsidiary of Singapore Airlines?\",\n",
    "        \"answer\": \"Scoot is the low-cost carrier that operates as a wholly owned subsidiary of Singapore Airlines.\",\n",
    "        \"tokens\": [\" Sco\", \"ot\", \" is\", ..., \" Airlines\", \".\", \"\\n\"],\n",
    "        \"supporting_answers\": [\n",
    "            \"As a wholly owned subsidiary of Singapore Airlines, <answer> Scoot </answer> stands as a low-cost carrier that revolutionized air travel in the region.\",\n",
    "            \"Scoot, a subsidiary of <answer> Singapore Airlines </answer> , is the low-cost carrier that operates under the same brand.\",\n",
    "            \"<answer> Scoot </answer> is the low-cost carrier that operates as a wholly owned subsidiary of Singapore Airlines.\",\n",
    "            \"Singapore Airlines operates a low-cost subsidiary named <answer> Scoot </answer> , offering affordable and efficient air travel options to passengers.\"\n",
    "        ],\n",
    "        \"supporting_tokens\": [\n",
    "            [\" As\", \" a\", ..., \".\", \"<answer>\"],\n",
    "            [\" Sco\", \"ot\", ..., \" brand\", \".\", \"\\n\"],\n",
    "            [\"<answer>\", \" Sco\", ..., \".\", \"\\n\"],\n",
    "            [\" Singapore\", \" Airlines\", ..., \".\", \"\\n\"]\n",
    "        ],\n",
    "        \"supporting_probabilities\": [\n",
    "            [0.0029233775567263365, 0.8621460795402527, ..., 0.018515007570385933],\n",
    "            [0.42073577642440796, 0.9999748468399048, ..., 0.9166142344474792],\n",
    "            [0.3258324861526489, 0.9969879984855652, ..., 0.921079695224762],\n",
    "            [0.11142394691705704, 0.960810661315918, ..., 0.9557166695594788]\n",
    "        ],\n",
    "        \"is_correct\": true\n",
    "    },\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "]\n",
    "```\n",
    "\n",
    "### Dane\n",
    "Dane dostępne dla Ciebie w tym zadaniu to:\n",
    "\n",
    "* `train.json` - zbiór danych zawierający 2967 pytań oraz odpowiedzi.\n",
    "* `valid.json` - 990 dodatkowych pytań.\n",
    "\n",
    "\n",
    "### Kryterium Oceny\n",
    "\n",
    "ROC AUC (ang. *Receiver Operating Characteristic Area Under Curve*) to miara jakości klasyfikatora binarnego. Pokazuje zdolność modelu do odróżniania między dwiema klasami - tutaj halucynacją (false) i poprawną odpowiedzią (true). \n",
    "\n",
    "- **ROC (Receiver Operating Characteristic)**: Wykres pokazujący zależność między *True Positive Rate* (czułość) a *False Positive Rate* (1-specyficzność) przy różnych progach decyzyjnych.\n",
    "- **AUC (Area Under Curve)**: Pole pod wykresem ROC, które przyjmuje wartości od 0 do 1:\n",
    "  - **1.0**: Model perfekcyjny.\n",
    "  - **0.5**: Model losowy (brak zdolności do odróżniania klas).\n",
    "\n",
    "Im wyższa wartość AUC, tym lepiej model radzi sobie z klasyfikacją.\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy 0 a 100 punktów. Wynik będzie skalowany liniowo w zależności od wartości ROC AUC:\n",
    "\n",
    "- **ROC AUC ≤ 0.7**: 0 punktów.\n",
    "- **ROC AUC ≥ 0.82**: 100 punktów.\n",
    "- **Wartości pomiędzy 0.7 a 0.82**: skalowane liniowo.\n",
    "\n",
    "Wzór na wynik:  \n",
    "$$\n",
    "\\text{Punkty} = \n",
    "\\begin{cases} \n",
    "0 & \\text{dla } \\text{ROC AUC} \\leq 0.7 \\\\\n",
    "100 \\times \\frac{\\text{ROC AUC} - 0.7}{0.82 - 0.7} & \\text{dla } 0.7 < \\text{ROC AUC} < 0.82 \\\\\n",
    "100 & \\text{dla } \\text{ROC AUC} \\geq 0.82\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "## Ograniczenia\n",
    "* Twoje rozwiazanie będzie testowane na Platformie Konkursowej bez dostępu do internetu oraz w środowisku bez GPU.\n",
    "* Ewaluacja Twojego finalnego rozwiązania na Platformie Konkursowej nie może trwać dłużej niż 5 minut bez GPU.\n",
    "* Lista dopuszczalnych bibliotek: `xgboost`, `scikit-learn`, `numpy`, `pandas`, `matplotlib`.\n",
    "\n",
    "\n",
    "## Pliki Zgłoszeniowe\n",
    "Ten notebook uzupełniony o Twoje rozwiązanie (patrz funkcja `predict_hallucinations`).\n",
    "\n",
    "## Ewaluacja\n",
    "Pamiętaj, że podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`.\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy 0 a 100 punktów. Liczba punktów, którą zdobędziesz, będzie wyliczona na (tajnym) zbiorze testowym na Platformie Konkursowej na podstawie wyżej wspomnianego wzoru, zaokrąglona do liczby całkowitej. Jeśli Twoje rozwiązanie nie będzie spełniało powyższych kryteriów lub nie będzie wykonywać się prawidłowo, otrzymasz za zadanie 0 punktów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kod Startowy\n",
    "W tej sekcji inicjalizujemy środowisko poprzez zaimportowanie potrzebnych bibliotek i funkcji. Przygotowany kod ułatwi Tobie efektywne operowanie na danych i budowanie właściwego rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "\n",
    "FINAL_EVALUATION_MODE = True  # W czasie sprawdzania twojego rozwiązania, zmienimy tą wartość na True\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def download_data(train=(\"1TGEDaxw4GKfSq0fpqSk0wRpUSc8GgZN0\", \"train.json\"),\n",
    "                  valid=(\"1qrr7bZk6Uct8DeC-V8Bc1qD5su56ryFd\", \"valid.json\")):\n",
    "    \"\"\"Pobiera zbiór danych z Google Drive i zapisuje go w folderze 'data'.\"\"\"\n",
    "    import gdown\n",
    "    \n",
    "    # Utwórz lub zresetuj folder 'data'\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    else:\n",
    "        shutil.rmtree('data')\n",
    "        os.makedirs('data')\n",
    "\n",
    "    GDRIVE_DATA = [train, valid]\n",
    "    \n",
    "    for file_id, file_name in GDRIVE_DATA:        \n",
    "        # Pobierz plik z Google Drive i zapisz go w folderze 'data'\n",
    "        url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        output = f'data/{file_name}'\n",
    "        gdown.download(url, output, quiet=False)\n",
    "        \n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "\n",
    "# Pobierz dane tylko jeśli nie jesteś w trybie FINAL_EVALUATION_MODE\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    download_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ładowanie Danych\n",
    "Za pomocą poniższego kodu dane zostaną wczytane i odpowiednio przygotowane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wszystkie przykłady treningowe: 2967\n",
      "Wszystkie przykłady walidacyjne: 990\n"
     ]
    }
   ],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "\n",
    "def load_data(folder='data'):\n",
    "    # Wczytaj dane z plików\n",
    "    train_path = os.path.join(folder, 'train.json')\n",
    "    valid_path = os.path.join(folder, 'valid.json')\n",
    "    \n",
    "    with open(train_path, 'r') as f:\n",
    "        train = json.load(f)\n",
    "    with open(valid_path, 'r') as f:\n",
    "        valid = json.load(f)\n",
    "\n",
    "    return train, valid\n",
    "\n",
    "train, valid = load_data(\"data\")\n",
    "\n",
    "print(f\"\\nWszystkie przykłady treningowe: {len(train)}\")\n",
    "print(f\"Wszystkie przykłady walidacyjne: {len(valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelowe Rozwiązanie\n",
    "\n",
    "Poniższe rozwiązanie przechodzi przez cały proces uczenia maszynowego, od eksploracji danych po ewaluację modelu na zbiorze walidacyjnym.\n",
    "Jako klasyfikator wykorzystano model XGBoost.\n",
    "Do uzyskania dobrych wyników wymagane jest stworzenie odpowiednich cech, które pozwolą na wykrycie halucynacji w odpowiedziach modelu językowego.\n",
    "\n",
    "Rozwiązanie zostało podzielone na poszczególne kroki:\n",
    "\n",
    "* [1. Eksploracja danych](#Eksploracja-danych)\n",
    "* [2. Ekstrakcja cech](#Ekstrakcja-cech)\n",
    "  * [2.1 Cechy statystyczne](#2.1-Cechy-statystyczne)\n",
    "  * [2.2 Cechy semantyczne](#2.2-Cechy-semantyczne)\n",
    "  * [2.3 Spójność odpowiedzi alternatywnych](#2.3-Spójność-odpowiedzi-alternatywnych)\n",
    "  * [2.4 Styl i struktura odpowiedzi](#2.4-Styl-i-struktura-odpowiedzi)\n",
    "  * [2.5 Typ pytania](#2.5-Typ-pytania)\n",
    "  * [2.6 Analiza prawdopodobieństw odpowiedzi](#2.6-Analiza-prawdopodobieństw-odpowiedzi)\n",
    "* [3. Agregacja cech](#3.-Agregacja-cech)\n",
    "* [4. Klasa do trenowania modelu](#4.-Klasa-do-trenowania-modelu)\n",
    "* [5. Klasa do ewaluacji modelu](#5.-Klasa-do-ewaluacji-modelu)\n",
    "* [6. Demo - cykl uczenia maszynowego](#6.-Demo---cykl-uczenia-maszynowego)\n",
    "  * [6.1 Przygotowanie danych](#6.1-Przygotowanie-danych)\n",
    "  * [6.2 Trenowanie modelu](#6.2-Trenowanie-modelu)\n",
    "  * [6.3 Ewaluacja modelu na zbiorze walidacyjnym](#6.3-Ewaluacja-modelu-na-zbiorze-walidacyjnym)\n",
    "* [7. (Opcjonalne) Ocena podzbiorów cech](#7.-(Opcjonalne)-Ocena-podzbiorów-cech)\n",
    "* [8. Ocena rozwiązania](#8.-Ocena-rozwiązania)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime\n",
    "import joblib\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "from typing import List, Optional\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Eksploracja danych\n",
    "\n",
    "\n",
    "TODO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ekstrakcja cech\n",
    "\n",
    "W ramach ekstrakcji cech zastosowano podejście oparte na analizie odpowiedzi modelu językowego.\n",
    "Zamieniamy odpowiedzi na numeryczne cechy, które zawierają informacje odnośnie halucynacji i mogą być wykorzystane do trenowania naszego klasyfikatora XGBoost.\n",
    "\n",
    "Poszczególne typy cech zostały wyeksportowane:\n",
    "* [2.1 Cechy statystyczne](#2.1-Cechy-statystyczne)\n",
    "* [2.2 Cechy semantyczne](#2.2-Cechy-semantyczne)\n",
    "* [2.3 Spójność odpowiedzi alternatywnych](#2.3-Spójność-odpowiedzi-alternatywnych)\n",
    "* [2.4 Styl i struktura odpowiedzi](#2.4-Styl-i-struktura-odpowiedzi)\n",
    "* [2.5 Typ pytania](#2.5-Typ-pytania)\n",
    "* [2.6 Analiza prawdopodobieństw odpowiedzi](#2.6-Analiza-prawdopodobieństw-odpowiedzi)\n",
    "\n",
    "\n",
    "**Disclaimer** - ekstrakcja cech to proces, który jest zarówno czasochłonny, jak i kreatywny. Podczas tego procesu **nie ma jednej, uniwersalnej \"poprawnej\" odpowiedzi**. W zależności od kontekstu i założeń modelu, różni specjaliści mogą wybrać różne cechy, które uznają za najistotniejsze dla rozwiązywanego problemu. Dlatego wyniki uzyskane przez różnych ekspertów w tej samej dziedzinie mogą się różnić, a różne podejścia mogą prowadzić do równie dobrych rezultatów.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:742/0*eySmc2fSF96yXIW0.png\" alt=\"Feature eng\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "Warto pamiętać, że proces ekstrakcji cech jest dynamiczny, zależy od danych, algorytmu oraz celu modelu. Często wymaga iteracyjnego dostosowywania i testowania różnych kombinacji cech w celu znalezienia najlepszej reprezentacji danych, która pozwoli uzyskać jak najdokładniejsze wyniki.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cechy statystyczne\n",
    "\n",
    "Definiujemy klasę `StatisticalFeatureExtractor`, której zadaniem jest wyliczanie różnych statystycznych cech na podstawie prawdopodobieństw tokenów uzyskanych dla odpowiedzi wspierających. \n",
    "\n",
    "Kod oblicza:\n",
    "- **Minimum Token Probability (`mtp`)** – najmniejsze prawdopodobieństwo przypisane któremuś tokenowi, co może wskazywać na niepewność modelu.\n",
    "- **Average Token Probability (`avgtp`)** – średnie prawdopodobieństwo wszystkich tokenów, reprezentujące ogólną pewność modelu.\n",
    "- **Maximum Probability Deviation (`mpd`)** – różnica między najwyższym a najniższym prawdopodobieństwem, mierząca zmienność w pewności modelu.\n",
    "<!-- - **Minimum Probability Spread (`mps`)** – minimalna rozpiętość prawdopodobieństw, najmniejsza różnica między najwyższym a najniższym prawdopodobieństwem, mierząca zmienność w pewności modelu. -->\n",
    "- **Generalized Negative Log-Likelihood (`g_nll`)** – uśredniona wartość ujemnego logarytmu prawdopodobieństwa, pozwalająca ocenić ogólną wiarygodność odpowiedzi.\n",
    "- **Variance of Token Probabilities (`var_prob`)** – wariancja prawdopodobieństw, która pokazuje rozrzut wartości i potencjalne niespójności.\n",
    "\n",
    "Te cechy są obliczane zarówno dla całej odpowiedzi, jak i dla tokenów znajdujących się wewnątrz znaczników `<answer>...</answer>`, które są kluczowymi frazami dla spójności odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Klasa wyciągająca statystyczne cechy, takie jak minimalne/średnie prawdopodobieństwo tokenu,\n",
    "    uogólniona negatywna logarytmiczna funkcja wiarygodności, wariancja prawdopodobieństw itd.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Konstruktor klasy - w tym przypadku nie musimy inicjalizować żadnych parametrów,\n",
    "        # dlatego używamy 'pass'\n",
    "        pass\n",
    "\n",
    "    def extract_features(self, sample: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Funkcja oblicza i zwraca słownik z cechami statystycznymi dla pojedynczej próbki danych.\n",
    "        \n",
    "        Sample to słownik zawierający dane z klucza \"supporting_probabilities\" w formacie\n",
    "                       listy list z prawdopodobieństwami tokenów.\n",
    "        Zwraca słownik zawierający wyliczone cechy statystyczne.\n",
    "        \"\"\"\n",
    "        # Inicjalizacja pustego słownika, w którym zapiszemy obliczone cechy\n",
    "        features = {}\n",
    "\n",
    "        supporting_probabilities = sample.get(\"supporting_probabilities\", [])\n",
    "        \n",
    "        # Iterujemy po liście, gdzie każdy element (probs) to lista prawdopodobieństw tokenów\n",
    "        for idx, probs in enumerate(supporting_probabilities):\n",
    "            probs_array = np.array(probs)\n",
    "\n",
    "            # Obliczamy minimalne i średnie prawdopodobieństwo tokenu\n",
    "            features[f'[stats] mtp_{idx}'] = np.min(probs_array)\n",
    "            features[f'[stats] avgtp_{idx}'] = np.mean(probs_array)\n",
    "            \n",
    "            # MPD = maksymalne prawdopodobieństwo - minimalne prawdopodobieństwo\n",
    "            mpd = np.max(probs_array) - np.min(probs_array)\n",
    "            features[f'[stats] mpd_{idx}'] = mpd\n",
    "            \n",
    "            # Obliczamy uogólnioną negatywną logarytmiczną funkcję wiarygodności (g_nll)\n",
    "            # Dodajemy małą wartość (1e-12) aby uniknąć logarytmu z 0\n",
    "            features[f'[stats] g_nll_{idx}'] = -np.sum(np.log(probs_array + 1e-12)) / len(probs_array)\n",
    "            \n",
    "            # Obliczamy wariancję prawdopodobieństw, która pokazuje rozrzut wartości\n",
    "            features[f'[stats] var_prob_{idx}'] = np.var(probs_array)\n",
    "\n",
    "        # Zwracamy słownik zawierający wszystkie obliczone cechy\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykładowe statystyczne atrybuty wygenerowane dla pojedynczej próbki danych (4 odpowiedzi alternatywne):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[stats] mtp_0': np.float64(0.12518402934074402),\n",
       " '[stats] avgtp_0': np.float64(0.6897126727160954),\n",
       " '[stats] mpd_0': np.float64(0.8742903172969818),\n",
       " '[stats] g_nll_0': np.float64(0.5484811111794412),\n",
       " '[stats] var_prob_0': np.float64(0.09952294183850739),\n",
       " '[stats] mtp_1': np.float64(0.03225848451256752),\n",
       " '[stats] avgtp_1': np.float64(0.8235318513157276),\n",
       " '[stats] mpd_1': np.float64(0.9677380584180355),\n",
       " '[stats] g_nll_1': np.float64(0.3563048823101568),\n",
       " '[stats] var_prob_1': np.float64(0.07862690426840574),\n",
       " '[stats] mtp_2': np.float64(0.38345128297805786),\n",
       " '[stats] avgtp_2': np.float64(0.8664189595164675),\n",
       " '[stats] mpd_2': np.float64(0.616548478603363),\n",
       " '[stats] g_nll_2': np.float64(0.18432452216302958),\n",
       " '[stats] var_prob_2': np.float64(0.043560149150476284),\n",
       " '[stats] mtp_3': np.float64(0.003265992272645235),\n",
       " '[stats] avgtp_3': np.float64(0.6464600048826209),\n",
       " '[stats] mpd_3': np.float64(0.9966791714541614),\n",
       " '[stats] g_nll_3': np.float64(0.8194122449444601),\n",
       " '[stats] var_prob_3': np.float64(0.10198290290676144)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train[0]\n",
    "StatisticalFeatureExtractor().extract_features(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cechy semantyczne\n",
    "\n",
    "Definiujemy klasę `SemanticFeatureExtractor`, której zadaniem jest związków semantycznych między pytaniem, główną odpowiedzią i alternatywnymi odpowiedziami. Jej celem jest wykrycie potencjalnych niespójności i ocenienie jakości generowanych odpowiedzi za pomocą technik przetwarzania języka naturalnego.  \n",
    "\n",
    "Kluczowe wyodrębnione cechy to:\n",
    "1. **Bag-of-Words Overlap (`bow_unique_ratio`)**  \n",
    "   - Oblicza stosunek unikalnych słów w `<answer>` do całkowitej liczby słów w alternatywnych odpowiedziach.  \n",
    "   - Wysoka wartość oznacza większą różnorodność słownictwa ([lexical diversity](https://en.wikipedia.org/wiki/Lexical_diversity)).  \n",
    "\n",
    "2. **TF-IDF Average (`tfidf_avg`)**  \n",
    "   - Mierzy średnią wartość `TF-IDF` ([Term Frequency-Inverse Document Frequency](https://pl.wikipedia.org/wiki/TFIDF)) dla odpowiedzi wspierających.  \n",
    "   - Ocenia ważność słów w kontekście całego zbioru danych.  \n",
    "\n",
    "3. **Question-Answer Cosine Similarity (`qa_cosine_similarity`)**  \n",
    "   - Mierzy podobieństwo semantyczne pytania i głównej odpowiedzi za pomocą podobieństwa cosinusów ([cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)) wektorów `TF-IDF` policzonych dla pytania i odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderForSemantic:\n",
    "    \"\"\"\n",
    "    Klasa pomocnicza do ładowania danych z plików JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_path: str = \"./data\"):\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def load_split(self, split: str = \"train\"):\n",
    "        \"\"\"\n",
    "        Ładuje podzbiór danych (train/valid/test) z pliku JSON.\n",
    "\n",
    "        :param split: Nazwa podzbioru (\"train\", \"valid\" lub \"test\").\n",
    "        :return: Lista próbek danych w formacie JSON.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.base_path, f\"{split}.json\")\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Plik z danymi nie został znaleziony: {file_path}\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "class SemanticFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Klasa odpowiedzialna za wyodrębnianie cech semantycznych:\n",
    "    - Pokrycie Bag-of-Words w sekcjach <answer>\n",
    "    - Podobieństwo na bazie TF-IDF\n",
    "    - Cosinusowe podobieństwo między pytaniem a odpowiedzią\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 vectorizer: Optional[TfidfVectorizer] = None, \n",
    "                 base_data_path: str = \"./data\"):\n",
    "        \"\"\"\n",
    "        Konstruktor klasy. Jeśli nie podano `vectorizer`, dane treningowe zostaną załadowane \n",
    "        i wektor TF-IDF zostanie dopasowany automatycznie.\n",
    "        \"\"\"\n",
    "        if vectorizer is None:\n",
    "            # Automatyczne ładowanie danych treningowych i budowanie korpusu\n",
    "            loader = DataLoaderForSemantic(base_path=base_data_path)\n",
    "            train_samples = loader.load_split(\"train\")\n",
    "            corpus = self._build_corpus_from_data(train_samples)\n",
    "            self.vectorizer = self.fit_vectorizer(corpus)\n",
    "            self.is_fitted = True\n",
    "        else:\n",
    "            self.vectorizer = vectorizer\n",
    "            self.is_fitted = True\n",
    "\n",
    "    @classmethod\n",
    "    def fit_vectorizer(cls, corpus: List[str]) -> TfidfVectorizer:\n",
    "        \"\"\"\n",
    "        Tworzy i dopasowuje wektorizer TF-IDF na podstawie dostarczonego korpusu tekstów.\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit(corpus)\n",
    "        return vectorizer\n",
    "\n",
    "    def _build_corpus_from_data(self, data_samples: List[dict]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Buduje korpus tekstowy na podstawie pytań, głównych odpowiedzi i odpowiedzi alternatywnych.\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for sample in data_samples:\n",
    "            corpus.append(self._preprocess_text(sample.get(\"question\", \"\")))\n",
    "            corpus.append(self._preprocess_text(sample.get(\"answer\", \"\")))\n",
    "            for sup_ans in sample.get(\"supporting_answers\", []):\n",
    "                corpus.append(self._preprocess_text(sup_ans))\n",
    "        return corpus\n",
    "\n",
    "    def _extract_answer_contents(self, supporting_answers: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Ekstrakcja treści zamkniętych w tagach <answer> z listy tekstów alternatywnych odpowiedzi.\n",
    "        \"\"\"\n",
    "        extracted = []\n",
    "        for ans in supporting_answers:\n",
    "            matches = re.findall(r\"<answer>(.*?)</answer>\", ans, re.IGNORECASE | re.DOTALL)\n",
    "            extracted.extend(matches)\n",
    "        return extracted\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Wstępne przetwarzanie surowego tekstu wejściowego: konwersja na małe litery i usunięcie znaków interpunkcyjnych.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def extract_features(self, sample: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Główna funkcja ekstrakcji cech semantycznych dla pojedynczej próbki danych.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Pobieramy odpowiedzi alternatywne\n",
    "        supporting_answers = sample.get(\"supporting_answers\", [])\n",
    "        extracted_answers = self._extract_answer_contents(supporting_answers)\n",
    "        preprocessed_answers = [self._preprocess_text(ans) for ans in extracted_answers]\n",
    "\n",
    "        # **Bag-of-Words Overlap**\n",
    "        all_words = ' '.join(preprocessed_answers).split()\n",
    "        word_counts = Counter(all_words)\n",
    "        unique_words = len(word_counts)\n",
    "        total_words = len(all_words)\n",
    "        features['[sem] bow_unique_ratio'] = unique_words / total_words if total_words > 0 else 0\n",
    "\n",
    "        # **Średnia wartość TF-IDF**\n",
    "        if preprocessed_answers and self.is_fitted:\n",
    "            tfidf_matrix = self.vectorizer.transform(preprocessed_answers)\n",
    "            avg_tfidf = tfidf_matrix.mean(axis=0).A1\n",
    "            features['[sem] tfidf_avg'] = float(avg_tfidf.mean())\n",
    "        else:\n",
    "            features['[sem] tfidf_avg'] = 0.0\n",
    "\n",
    "        # **Cosinusowe podobieństwo między pytaniem a odpowiedzią**\n",
    "        question = self._preprocess_text(sample.get(\"question\", \"\"))\n",
    "        main_answer = self._preprocess_text(sample.get(\"answer\", \"\"))\n",
    "        combined_corpus = [question, main_answer]\n",
    "        \n",
    "        if self.is_fitted:\n",
    "            tfidf_q_a = self.vectorizer.transform(combined_corpus)\n",
    "            similarity = cosine_similarity(tfidf_q_a[0:1], tfidf_q_a[1:2])[0][0]\n",
    "            features['[sem] qa_cosine_similarity'] = float(similarity)\n",
    "        else:\n",
    "            features['[sem] qa_cosine_similarity'] = 0.0\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Spójność odpowiedzi alternatywnych\n",
    "\n",
    "Definiujemy klasę `CrossAnswerConsistencyExtractor`, której zadaniem jest ocena zgodności odpowiedzi alternatywnych z główną odpowiedzią.\n",
    "Analizując zawartość pomiędzy tagami `<answer>...</answer>` w różnych odpowiedziach alternatywnych, identyfikuje wzorce, które mogą wskazywać na wiarygodność lub niepewność głównej odpowiedzi.\n",
    "\n",
    "Kluczowe wyodrębnione cechy to:\n",
    "- **Liczba unikalnych odpowiedzi (`num_unique_answers`)**: Liczy, ile różnych odpowiedzi występuje w odpowiedziach alternatywnych. Większa różnorodność może wskazywać na niepewność w odpowiedzi.\n",
    "- **Liczba najczęstszej odpowiedzi (`most_common_answer_count`)**: Określa częstotliwość najczęściej powtarzanej odpowiedzi, odzwierciedlając zgodność odpowiedzi alternatywnych.\n",
    "- **Wskaźnik zgody (`agreement_ratio`)**: Oblicza proporcję odpowiedzi alternatywnych, które zgadzają się z najczęściej powtarzaną odpowiedzią, podkreślając poziom konsensusu.\n",
    "\n",
    "Te cechy pomagają określić, czy odpowiedzi alternatywne zbiegają się do jednej odpowiedzi, czy prezentują sprzeczne informacje, co jest kluczowe w wykrywaniu potencjalnych halucynacji.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAnswerConsistencyExtractor:\n",
    "    \"\"\"\n",
    "    Ektrakcja cech związanych ze spójnością odpowiedzi alternatywnych.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _extract_answer_contents(self, supporting_answers: list) -> list:\n",
    "        extracted = []\n",
    "        for ans in supporting_answers:\n",
    "            matches = re.findall(r\"<answer>(.*?)</answer>\", ans, re.IGNORECASE | re.DOTALL)\n",
    "            extracted.extend(matches)\n",
    "        return extracted\n",
    "\n",
    "    def extract_features(self, sample: dict) -> dict:\n",
    "        features = {}\n",
    "        supporting_answers = sample.get(\"supporting_answers\", [])\n",
    "        extracted_answers = self._extract_answer_contents(supporting_answers)\n",
    "\n",
    "        answer_counts = Counter(extracted_answers)\n",
    "        num_unique_answers = len(answer_counts)\n",
    "        features['[cross] num_unique_answers'] = num_unique_answers\n",
    "\n",
    "        if answer_counts:\n",
    "            most_common_count = answer_counts.most_common(1)[0][1]\n",
    "        else:\n",
    "            most_common_count = 0\n",
    "        features['[cross] most_common_answer_count'] = most_common_count\n",
    "\n",
    "        total_supporting = len(supporting_answers)\n",
    "        agreement_ratio = most_common_count / total_supporting if total_supporting > 0 else 0\n",
    "        features['[cross] agreement_ratio'] = agreement_ratio\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Styl i struktura odpowiedzi\n",
    "\n",
    "Definiujemy klasę `StyleFeatureExtractor`, której zadaniem jest analiza cech stylistycznych i strukturalnych odpowiedzi generowanych przez model językowy. Dzięki badaniu struktury zdań i podobieństw leksykalnych dostarcza informacji o spójności odpowiedzi alternatywnych z główną odpowiedzią.\n",
    "\n",
    "Kluczowe wyodrębnione cechy to:  \n",
    "1. **Zmienność długości zdań (`sentence_length_variance`)** – mierzy, jak bardzo różnią się długości zdań w głównej i odpowiedziach alternatywnych, co wskazuje na spójność stylistyczną.  \n",
    "2. **Średnia długość zdania (`average_sentence_length`)** – oblicza średnią liczbę słów na zdanie, co odzwierciedla złożoność i rozwlekłość odpowiedzi.  \n",
    "3. **Średnia długość najdłuższego wspólnego podciągu (`average_lcs`)** – określa średnią długość najdłuższego wspólnego podciągu między główną odpowiedzią a każdą z odpowiedzi wspierających, co pozwala ocenić ich podobieństwo leksykalne.  \n",
    "\n",
    "Dzięki tym cechom można skutecznie ocenić stylistyczną spójność odpowiedzi, upewniając się, że wszystkie części wypowiedzi utrzymują jednolitą strukturę narracyjną.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Oblicza cechy związane ze stylem i strukturą odpowiedzi: \n",
    "    - zmienność długości zdań\n",
    "    - średnią długość zdania\n",
    "    - średnią długość najdłuższego wspólnego podciągu (LCS) względem odpowiedzi alternatywnych\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    def _compute_lcs(self, str1: str, str2: str) -> int:\n",
    "        \"\"\"\n",
    "        Oblicza długość najdłuższego wspólnego podciągu (LCS) między dwoma tekstami.\n",
    "        Zwraca długość LCS jako liczbę całkowitą.\n",
    "        \"\"\"\n",
    "        matcher = SequenceMatcher(None, str1, str2)\n",
    "        match = matcher.find_longest_match(0, len(str1), 0, len(str2))\n",
    "        return match.size\n",
    "\n",
    "    def extract_features(self, sample: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Zwraca cechy stylu i struktury odpowiedzi dla pojedynczej próbki danych.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        main_answer = sample.get(\"answer\", \"\")\n",
    "        supporting_answers = sample.get(\"supporting_answers\", [])\n",
    "\n",
    "        main_sentences = self._split_into_sentences(main_answer)\n",
    "        if not main_sentences:\n",
    "            main_sentences = [main_answer]\n",
    "\n",
    "        # \n",
    "        # We'll also gather supporting sentences to combine them for variance\n",
    "        supporting_sentences = []\n",
    "        for supp_ans in supporting_answers:\n",
    "            supporting_sentences.extend(self._split_into_sentences(supp_ans))\n",
    "\n",
    "        all_sentences = main_sentences + supporting_sentences\n",
    "        sentence_lengths = [len(s.split()) for s in all_sentences]\n",
    "\n",
    "        if len(sentence_lengths) > 1:\n",
    "            var_len = float(np.var(sentence_lengths))\n",
    "            avg_len = float(np.mean(sentence_lengths))\n",
    "        elif len(sentence_lengths) == 1:\n",
    "            var_len = 0.0\n",
    "            avg_len = float(sentence_lengths[0])\n",
    "        else:\n",
    "            var_len = 0.0\n",
    "            avg_len = 0.0\n",
    "\n",
    "        features['[style] sentence_length_variance'] = var_len\n",
    "        features['[style] average_sentence_length'] = avg_len\n",
    "\n",
    "        # Average LCS between main answer and supporting answers\n",
    "        lcs_lengths = []\n",
    "        for supp_ans in supporting_answers:\n",
    "            lcs_length = self._compute_lcs(main_answer, supp_ans)\n",
    "            lcs_lengths.append(lcs_length)\n",
    "\n",
    "        if lcs_lengths:\n",
    "            features['[style] average_lcs'] = float(np.mean(lcs_lengths))\n",
    "        else:\n",
    "            features['[style] average_lcs'] = 0.0\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Typ pytania\n",
    "\n",
    "Definiujemy klasę `QuestionTypeExtractor`, której zadaniem jest określenie typu pytania na podstawie jego początkowego słowa.\n",
    "Tworzymy binarne cechy (one-hot) dla każdego z typu pytań - **Who (Kto), What (Co), Where (Gdzie), When (Kiedy), Why (Dlaczego), How (Jak) oraz Which (Który)**, które zawierają kontekst wpływający na odpowiedź.\n",
    "Te cechy stanowią dodatkowe informacje odnośnie wrodzonego charakteru pytania, co zwiększa dokładność wykrywania halucynacji.\n",
    "\n",
    "Kluczowe wyodrębnione cechy to:\n",
    "- `is_who`: Czy pytanie dotyczy osoby?\n",
    "- `is_what`: Czy pytanie prosi o definicję lub opis?\n",
    "- `is_where`: Czy pytanie dotyczy miejsca?\n",
    "- `is_when`: Czy pytanie odnosi się do czasu?\n",
    "- `is_why`: Czy pytanie szuka przyczyny lub wyjaśnienia?\n",
    "- `is_how`: Czy pytanie dotyczy procesu lub metody?\n",
    "- `is_which`: Czy pytanie wymaga wyboru między opcjami?\n",
    "\n",
    "Do identyfikacji pytania używamy [regexów](https://regexone.com/) do dopasowania pierwszego słowa pytania do wzorców odpowiadających różnym typom pytań.\n",
    "\n",
    "Przykładowo `'^\\s*who\\b'` dopasowuje pytania zaczynające się od słowa \"Who\"z opcjonalnymi białymi znakami (spacja, tabulator) na początku zdania. \n",
    "`'\\b'` oznacza granicę słowa, co oznacza, że dopasowanie musi być dokładne, a nie częściowe.\n",
    "Dodatkowo, wykorzystujemy funkcję `re.IGNORECASE` do ignorowania wielkości liter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionTypeFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Określa typ pytania za pomocą prostych heurystyk opartych na wzorcach.\n",
    "    Dla każdego pytania tworzy binarne cechy, które określają, czy pytanie jest danego typu.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            '[type]  is_who': re.compile(r'^\\s*who\\b', re.IGNORECASE),\n",
    "            '[type]  is_what': re.compile(r'^\\s*what\\b', re.IGNORECASE),\n",
    "            '[type]  is_where': re.compile(r'^\\s*where\\b', re.IGNORECASE),\n",
    "            '[type]  is_when': re.compile(r'^\\s*when\\b', re.IGNORECASE),\n",
    "            '[type]  is_why': re.compile(r'^\\s*why\\b', re.IGNORECASE),\n",
    "            '[type]  is_how': re.compile(r'^\\s*how\\b', re.IGNORECASE),\n",
    "            '[type]  is_which': re.compile(r'^\\s*which\\b', re.IGNORECASE)\n",
    "        }\n",
    "\n",
    "    def extract_features(self, sample: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Główna funkcja ekstrakcji typu pytania dla pojedynczej próbki danych.\n",
    "\n",
    "        Określa typ pytania na podstawie dopasowywania wzorców.\n",
    "        \"\"\"\n",
    "\n",
    "        features = {\n",
    "            '[type]  is_who': 0,\n",
    "            '[type]  is_what': 0,\n",
    "            '[type]  is_where': 0,\n",
    "            '[type]  is_when': 0,\n",
    "            '[type]  is_why': 0,\n",
    "            '[type]  is_how': 0,\n",
    "            '[type]  is_which': 0\n",
    "        }\n",
    "\n",
    "        question = sample.get(\"question\", \"\").strip().lower()\n",
    "        # Sprawdzamy, który wzorzec (pattern) pasuje do pytania\n",
    "        for feat_name, pattern in self.patterns.items():\n",
    "            if pattern.match(question):\n",
    "                features[feat_name] = 1\n",
    "                break\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.6 Analiza prawdopodobieństw odpowiedzi**  \n",
    "\n",
    "Definiujemy klasę `AnswerProbabilityFeatureExtractor`, której zadaniem jest wyodrębnienie cech związanych z prawdopodobieństwem tokenów wewnątrz tagów `<answer>...</answer>` znajdujących się w każdej alternatywnej odpowiedzi. Jego celem jest **ilościowe określenie pewności modelu** co do treści odpowiedzi oraz ocena, jak spójnie generowane tokeny odpowiedzi alternatywnych pokrywają się z główną odpowiedzią. W szczególności wykonuje on następujące kroki:\n",
    "\n",
    "\n",
    "**1. `find_answer_probability()`**  \n",
    "Lokalizuje tokeny pomiędzy `<answer>` i `</answer>` dla pojedynczej alternatywnej odpowiedzi i oblicza **średnie prawdopodobieństwo** tych tokenów.  \n",
    "\n",
    "**2. `generate_answer_probabilities()`**  \n",
    "Stosuje powyższą funkcję do wszystkich wspierających odpowiedzi i dzieli wyniki na dwie grupy:  \n",
    "- **Prawdopodobieństwa „Answer”** – gdy wyodrębniony tekst wewnątrz tagów `<answer>` znajduje się w głównej odpowiedzi, co sugeruje zgodnośc z główną odpowiedzią.  \n",
    "- **Prawdopodobieństwa „Other”** – gdy wyodrębniony tekst nie pojawia się w głównej odpowiedzi, co może sugerować rozbieżność.  \n",
    "\n",
    "**3. Agregacja statystyczna**  \n",
    "Oblicza szereg **statystyk** dla każdej z dwóch grup prawdopodobieństw, w tym:  \n",
    "- **Dla prawdopodobieństw „answer”**:  \n",
    "  - Minimum (`answer_min`)  \n",
    "  - Średnia (`answer_mean`)  \n",
    "  - Maksimum (`answer_max`)  \n",
    "  - Odchylenie standardowe (`answer_std`)  \n",
    "  - Liczność (`answer_len`)  \n",
    "\n",
    "- **Dla prawdopodobieństw „other”**:  \n",
    "  - Minimum (`other_min`)  \n",
    "  - Średnia (`other_mean`)  \n",
    "  - Maksimum (`other_max`)  \n",
    "  - Odchylenie standardowe (`other_std`)  \n",
    "  - Liczność (`other_len`)  \n",
    "\n",
    "**4. Statystyki dla każdej wspierającej odpowiedzi**  \n",
    "Dodatkowo, dla każdej z **czterech** wspierających odpowiedzi, ekstraktor oblicza podsumowanie statystyczne (minimum, średnia, maksimum, odchylenie standardowe i długość tablicy prawdopodobieństw), zapisując je w postaci `supporting_proba_0_min`, `supporting_proba_0_mean`, ..., aż do `supporting_proba_3_*`.  \n",
    "\n",
    "Te cechy pozwalają na ocenę nie tylko **pewności modelu** względem wygenerowanych fragmentów odpowiedzi, ale także stopnia ich **zgodności z główną odpowiedzią** oraz ogólnej zmienności prawdopodobieństw tokenów wśród wspierających odpowiedzi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerProbabilityFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Ekstrakcja specjalistycznych cech za pomocą analizy tokenów pomiędzy <answer>...</answer> \n",
    "    w każdej odpowiedzi alternatywnej, skupiając się na ich prawdopodobieństwach, zgodności z\n",
    "    główną odpowiedzią i podsumowaniach statystycznych.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _find_token_idx(self, token: str, tokens: list) -> int:\n",
    "        \"\"\"\n",
    "        Zwraca pierwszy indeks w 'tokens', który pasuje do 'token'.\n",
    "        Zwraca -1, jeśli indeks nie zostanie znaleziony.\n",
    "        \"\"\"\n",
    "        for i, t in enumerate(tokens):\n",
    "            if t == token:\n",
    "                return i\n",
    "        return -1\n",
    "\n",
    "    def _find_answer_probability(self, tokens: list, probabilities: list) -> (float, str):\n",
    "        \"\"\"\n",
    "        Znajduje średnie prawdopodobieństwo tokenów między <answer> i </answer> w 'tokens'.\n",
    "        Zwraca średnie_prawdopodobieństwo ze złączonym tekstem.\n",
    "        \"\"\"\n",
    "        idx_start = self._find_token_idx('<answer>', tokens)\n",
    "        idx_end = self._find_token_idx('</answer>', tokens)\n",
    "\n",
    "        # Jeśli indeksy są w złej kolejności, zamieniamy je miejscami\n",
    "        if idx_end < idx_start:\n",
    "            idx_start, idx_end = idx_end, idx_start\n",
    "\n",
    "        # Bierzemy prawdopodobieństwa dla tokenów ściśle między <answer> i </answer>\n",
    "        # (wykluczamy same tagi)\n",
    "        segment_probs = probabilities[idx_start+1:idx_end]\n",
    "        segment_text_tokens = tokens[idx_start+1:idx_end]\n",
    "\n",
    "        mean_prob = np.mean(segment_probs) if len(segment_probs) > 0 else 0.0\n",
    "        joined_text = ''.join(segment_text_tokens)  # np. \"MountEverest\" etc.\n",
    "        return (mean_prob, joined_text)\n",
    "\n",
    "    def _generate_answer_probabilities(self, sample: dict):\n",
    "        \"\"\"\n",
    "        Dla każdej odpowiedzi altenatywnej oblicza średnie prawdopodobieństwo tokenów między <answer>...</answer>.\n",
    "        Klasyfikuje je jako 'answer', jeśli ten tekst jest zawarty w głównej odpowiedzi, w przeciwnym razie 'other'.\n",
    "        \"\"\"\n",
    "        answer_probs = []\n",
    "        other_probs = []\n",
    "\n",
    "        supporting_answers = sample.get('supporting_answers', [])\n",
    "        supporting_tokens = sample.get('supporting_tokens', [])\n",
    "        supporting_probabilities = sample.get('supporting_probabilities', [])\n",
    "\n",
    "        main_ans_text = sample.get('answer', '')\n",
    "\n",
    "        for i in range(len(supporting_answers)):\n",
    "            tokens_i = supporting_tokens[i]\n",
    "            probs_i = supporting_probabilities[i]\n",
    "\n",
    "            mean_prob, extracted_text = self._find_answer_probability(tokens_i, probs_i)\n",
    "\n",
    "            # Jeśli extracted_text jest zawarty w głównej odpowiedzi, \n",
    "            # klasyfikujemy to jako 'answer', w przeciwnym razie 'other'\n",
    "            if extracted_text and extracted_text in main_ans_text:\n",
    "                answer_probs.append(mean_prob)\n",
    "            else:\n",
    "                other_probs.append(mean_prob)\n",
    "\n",
    "        return answer_probs, other_probs\n",
    "\n",
    "    def extract_features(self, sample: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Główna funkcja ekstrakcji typu pytania dla pojedynczej próbki danych.\n",
    "\n",
    "        Zwraca słownik cech opisujących pewność modelu dla segmentów <answer>...</answer>,\n",
    "        wraz z podsumowaniami statystycznymi dla prawdpodobieństw każdej z czterech alternatywnych odpowiedzi.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # 1. Oblicz prawdopodobieństwa dla 'answer' i 'other'\n",
    "        answer_probs, other_probs = self._generate_answer_probabilities(sample)\n",
    "        a_len = len(answer_probs)\n",
    "        o_len = len(other_probs)\n",
    "\n",
    "        features['[ansprob] answer_min'] = float(np.min(answer_probs)) if a_len else 0.0\n",
    "        features['[ansprob] answer_mean'] = float(np.mean(answer_probs)) if a_len else 0.0\n",
    "        features['[ansprob] answer_max'] = float(np.max(answer_probs)) if a_len else 0.0\n",
    "        features['[ansprob] answer_std'] = float(np.std(answer_probs)) if a_len else 0.0\n",
    "        features['[ansprob] answer_len'] = a_len\n",
    "\n",
    "        features['[ansprob] other_min'] = float(np.min(other_probs)) if o_len else 0.0\n",
    "        features['[ansprob] other_mean'] = float(np.mean(other_probs)) if o_len else 0.0\n",
    "        features['[ansprob] other_max'] = float(np.max(other_probs)) if o_len else 0.0\n",
    "        features['[ansprob] other_std'] = float(np.std(other_probs)) if o_len else 0.0\n",
    "        features['[ansprob] other_len'] = o_len\n",
    "\n",
    "        # 2. Dla każdej z czterech alternatywnych odpowiedzi zbierz statystyki\n",
    "        supporting_probs = sample.get('supporting_probabilities', [])\n",
    "        for i in range(4):\n",
    "            if i < len(supporting_probs):\n",
    "                arr = np.array(supporting_probs[i])\n",
    "                features[f'[ansprob] supporting_proba_{i}_min'] = float(arr.min())\n",
    "                features[f'[ansprob] supporting_proba_{i}_mean'] = float(arr.mean())\n",
    "                features[f'[ansprob] supporting_proba_{i}_max'] = float(arr.max())\n",
    "                features[f'[ansprob] supporting_proba_{i}_std'] = float(arr.std())\n",
    "                features[f'[ansprob] supporting_proba_{i}_len'] = arr.size\n",
    "            else:\n",
    "                # Jeśli w próbce jest mniej niż 4 wspierające odpowiedzi, wypełnij cechy zerami\n",
    "                features[f'[ansprob] supporting_proba_{i}_min'] = 0.0\n",
    "                features[f'[ansprob] supporting_proba_{i}_mean'] = 0.0\n",
    "                features[f'[ansprob] supporting_proba_{i}_max'] = 0.0\n",
    "                features[f'[ansprob] supporting_proba_{i}_std'] = 0.0\n",
    "                features[f'[ansprob] supporting_proba_{i}_len'] = 0\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agregacja cech\n",
    "\n",
    "Po uzyskaniu cech z różnych kategorii dla pojedynczej próbki, łączymy je razem w jeden skonsolidowany słownik cech, aby móc wykorzystać je do trenowania modelu klasyfikacji XGBoost.\n",
    "\n",
    "Celem klasy `FeatureAggregator` jest uproszczenie procesu ekstrakcji cech poprzez centralizację logiki wywoływania różnych ekstraktorów cech i łączenia wyników w jeden spójny zestaw cech. Dzięki temu kod staje się bardziej modularny i łatwiejszy do zarządzania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAggregator:\n",
    "    \"\"\"\n",
    "    Przeprowadza ekstrakcję wszystkich zestawów cech\n",
    "    Łączy je w jeden skonsolidowany słownik cech.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_data_path: str = \"./data\"):\n",
    "        \"\"\"\n",
    "        Inicjalizuje wszystkie potrzebne ekstraktory cech.\n",
    "        \"\"\"\n",
    "        self.extractors = {\n",
    "            \"statistical\": StatisticalFeatureExtractor(),\n",
    "            \"semantic\": SemanticFeatureExtractor(base_data_path=base_data_path),\n",
    "            \"cross_answer_consistency\": CrossAnswerConsistencyExtractor(),\n",
    "            \"style\": StyleFeatureExtractor(),\n",
    "            \"question_type\": QuestionTypeFeatureExtractor(),\n",
    "            \"answer_probability\": AnswerProbabilityFeatureExtractor()\n",
    "        }\n",
    "\n",
    "    def aggregate_features(self, sample: dict, feature_sets=\"all\") -> dict:\n",
    "        \"\"\"\n",
    "        Wywołuje odpowiednie ekstraktory cech na próbce.\n",
    "        Jeśli 'feature_sets' to \"all\", używa wszystkich ekstraktorów; \n",
    "        w przeciwnym razie używamy wszystkich poprawnych cech z listy.\n",
    "        \"\"\"\n",
    "        consolidated_features = {}\n",
    "\n",
    "        if feature_sets == \"all\":\n",
    "            selected_extractors = self.extractors.values()\n",
    "        elif isinstance(feature_sets, list):\n",
    "            selected_extractors = [self.extractors[name]\n",
    "                                   for name in feature_sets\n",
    "                                   if name in self.extractors]\n",
    "        else:\n",
    "            raise ValueError(\"feature_sets must be 'all' or a list of feature set names.\")\n",
    "\n",
    "        for extractor in selected_extractors:\n",
    "            feats = extractor.extract_features(sample)\n",
    "            consolidated_features.update(feats)\n",
    "\n",
    "        return consolidated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Klasa do trenowania modelu\n",
    "\n",
    "Do klasyfikacji używamy modelu [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier), który jest popularnym algorytmem uczenia maszynowego w konkursach Kaggle i w praktyce. XGBoost jest algorytmem gradient boostingowym, który łączy wiele słabych modeli w celu uzyskania silnego modelu predykcyjnego. Jest on wydajny, skalowalny i zapewnia dobre wyniki dla różnych problemów klasyfikacji i regresji.\n",
    "\n",
    "Klasa `Trainer` pozwala na:\n",
    "- Przeprowadzenia walidacji krzyżowej w celu optymalizacji hiperparametrów modelu [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). \n",
    "- Wytrenowanie modelu [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier) na podanym zbiorze danych treningowych i wykorzystuje zbiór walidacyjny do wczesnego zatrzymywania (ang. early stopping) w celu uniknięcia przeuczenia modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trenuje model XGBoost lub przeprowadza GridSearchCV na GradientBoostingClassifier.\n",
    "    Ostateczny model jest przechowywany w 'self.model' i może być zapisany/załadowany.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_aggregator: FeatureAggregator = None, model_dir = \"./models\"):\n",
    "        \"\"\"\n",
    "        Inicjalizuje obiekt trenera razem z agregatorem cech i katalogiem modelu do zapisu.\n",
    "        \"\"\"\n",
    "        if feature_aggregator is None:\n",
    "            feature_aggregator = FeatureAggregator()\n",
    "        self.feature_aggregator = feature_aggregator\n",
    "        self.model_dir = model_dir\n",
    "        os.makedirs(self.model_dir, exist_ok=True)  # Ensure model directory exists\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, train_data, valid_data, feature_sets=\"all\", verbose=True, do_grid_search=False):\n",
    "        \"\"\"\n",
    "        :param train_data: Lista próbek danych (słowników) do trenowania.\n",
    "        :param valid_data: Lista próbek danych (słowników) do walidacji.\n",
    "        :param feature_sets: \"all\" lub lista nazw zestawów cech dla agregatora.\n",
    "        :param verbose: Jeśli True, drukuje postęp i końcowe AUC.\n",
    "        :param do_grid_search: Jeśli True, przeprowadza GridSearchCV na GradientBoostingClassifier.\n",
    "                              Jeśli False, trenuje XGBClassifier z ustalonymi hiperparametrami i wczesnym zatrzymaniem.\n",
    "        \"\"\"\n",
    "        X_train, y_train = self._prepare_data(train_data, feature_sets)\n",
    "        X_valid, y_valid = self._prepare_data(valid_data, feature_sets)\n",
    "\n",
    "        if do_grid_search:\n",
    "            # GridSearch over GradientBoostingClassifier\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 300, 500, 1000],\n",
    "                'learning_rate': [0.01, 0.03, 0.05],\n",
    "                'max_depth': [1, 3, 5, 7],\n",
    "                'min_samples_split': [5, 10, 15],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'subsample': [0.2, 0.5, 0.8, 1.0],\n",
    "                'validation_fraction': [0.1],\n",
    "                'n_iter_no_change': [10, 20],\n",
    "                'tol': [1e-4, 1e-3]\n",
    "            }\n",
    "\n",
    "\n",
    "            base_model = GradientBoostingClassifier(\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=base_model,\n",
    "                param_grid=param_grid,\n",
    "                cv=3,\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=-1,\n",
    "                verbose=5 if verbose else 0\n",
    "            )\n",
    "\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_params = grid_search.best_params_\n",
    "            if verbose:\n",
    "                print(f\"\\n[Trainer] Best params from Grid Search: {best_params}\\n\")\n",
    "\n",
    "            self.model = GradientBoostingClassifier(\n",
    "                **best_params,\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            self.model.fit(X_train, y_train)\n",
    "        else:\n",
    "            # Use XGB with fixed hyperparameters & early stopping\n",
    "            self.model = XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.03,\n",
    "                max_depth=3,\n",
    "                min_child_weight=4,\n",
    "                subsample=0.5,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1,\n",
    "                gamma=0,\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='auc',\n",
    "                early_stopping_rounds=10,\n",
    "                verbosity=1\n",
    "            )\n",
    "            self.model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "        # Evaluate\n",
    "        if verbose:\n",
    "            # Training AUC\n",
    "            preds_train = self.model.predict_proba(X_train)[:, 1]\n",
    "            auc_train = roc_auc_score(y_train, preds_train)\n",
    "            print(f\"[Trainer] Training AUC: {auc_train:.4f}\")\n",
    "\n",
    "            # Validation AUC\n",
    "            preds_valid = self.model.predict_proba(X_valid)[:, 1]\n",
    "            auc_valid = roc_auc_score(y_valid, preds_valid)\n",
    "            print(f\"[Trainer] Validation AUC: {auc_valid:.4f}\")\n",
    "\n",
    "    def _prepare_data(self, dataset, feature_sets=\"all\"):\n",
    "        \"\"\"\n",
    "        Convert each sample into (X, y).\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for sample in dataset:\n",
    "            feats = self.feature_aggregator.aggregate_features(sample, feature_sets)\n",
    "            X.append(list(feats.values()))\n",
    "            y.append(int(sample.get('is_correct', 0)))\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def save_model(self, model_name=\"model\"):\n",
    "        if self.model is None:\n",
    "            print(\"[Trainer] No model to save.\")\n",
    "            return\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\")\n",
    "        filename = f\"{model_name}_{timestamp}.pkl\"\n",
    "        filepath = os.path.join(self.model_dir, filename)\n",
    "        joblib.dump(self.model, filepath)\n",
    "        print(f\"[Trainer] Model saved to {filepath}\")\n",
    "\n",
    "    def load_model(self, model_path=None):\n",
    "        if model_path is None:\n",
    "            files = os.listdir(self.model_dir)\n",
    "            pkl_files = [f for f in files if f.endswith('.pkl')]\n",
    "            if not pkl_files:\n",
    "                print(\"[Trainer] No .pkl files in model directory.\")\n",
    "                return None\n",
    "            pkl_files.sort(key=lambda f: os.path.getmtime(os.path.join(self.model_dir, f)), reverse=True)\n",
    "            model_path = os.path.join(self.model_dir, pkl_files[0])\n",
    "\n",
    "        self.model = joblib.load(model_path)\n",
    "        print(f\"[Trainer] Loaded model from {model_path}\")\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Klasa do ewaluacji modelu\n",
    "\n",
    "Do pełnego cyklu uczenia maszynowego musimy również ocenić skuteczność naszego modelu na zbiorze nie widzianym podczas treningu. W tym celu wykorzystujemy zbiór walidacyjny, który pozwala na sprawdzenie, jak dobrze nasz model generalizuje przewidywanie etykiet na nowych danych.\n",
    "\n",
    "Klasa `Evaluator` ocenia jakość zapisanego modelu XGBoost na podanym zbiorze danych. Używa modelu do prognozowania etykiet i oblicza miarę ROC AUC, która ocenia jakość klasyfikatora binarnego. Przed prognozowaniem, dane walidacyjne są przetwarzane tak samo jak dane treningowe, aby wyodrębnić cechy potrzebne do prognozowania.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Przeprowadza ewaluację wytrenowanego modelu XGB na danym zbiorze danych (np. walidacyjnym).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_aggregator: FeatureAggregator, model_dir=\"./models\"):\n",
    "        self.feature_aggregator = feature_aggregator\n",
    "        self.model_dir = model_dir\n",
    "        self.model = None\n",
    "\n",
    "    def evaluate(self, data, model_path=None, feature_sets=\"all\"):\n",
    "        \"\"\"\n",
    "        Zwraca wynik AUC dla danego modelu na podanym zbiorze danych.\n",
    "        Domyślnie używa najnowszego zapisanego modelu, jeśli model_path=None.\n",
    "        \"\"\"\n",
    "        model = self._load_model(model_path)\n",
    "        if model is None:\n",
    "            print(\"[Evaluator] No model to evaluate.\")\n",
    "            return 0.0\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        for sample in data:\n",
    "            feats = self.feature_aggregator.aggregate_features(sample, feature_sets)\n",
    "            X.append(list(feats.values()))\n",
    "            y.append(int(sample.get(\"is_correct\", 0)))\n",
    "\n",
    "        preds_proba = model.predict_proba(X)[:, 1]\n",
    "        auc_score_val = roc_auc_score(y, preds_proba)\n",
    "        return auc_score_val\n",
    "\n",
    "    def _load_model(self, model_path=None):\n",
    "        if model_path is None:\n",
    "            # Znajdź najnowszy plik modelu .pkl w katalogu model_dir\n",
    "            files = os.listdir(self.model_dir)\n",
    "            pkl_files = [f for f in files if f.endswith('.pkl')]\n",
    "            if not pkl_files:\n",
    "                print(\"[Evaluator] No .pkl model files in the directory.\")\n",
    "                return None\n",
    "            pkl_files.sort(key=lambda x: os.path.getmtime(os.path.join(self.model_dir, x)), reverse=True)\n",
    "            model_path = os.path.join(self.model_dir, pkl_files[0])\n",
    "\n",
    "        self.model = joblib.load(model_path)\n",
    "        print(f\"[Evaluator] Loaded model from {model_path}\")\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demo - Cykl uczenia maszynowego\n",
    "\n",
    "Poniżej znajduje się przykładowy kod, który używa zaimplementowane klasy i demonstruje pełny cykl uczenia maszynowego, od ekstrakcji cech do ewaluacji modelu na zbiorze walidacyjnym. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Wytrenowanie modelu\n",
    "\n",
    "W pierwszej kolejności tworzymy instancję klasy `FeatureAggregator`, która pozwala na ekstrakcję cech z danych treningowych. Następnie przekazujemy te cechy do klasy `Trainer`, która trenuje model [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) i optymalizuje hiperparametry za pomocą walidacji krzyżowej. Po zakończeniu trenowania, zapisujemy wytrenowany model do pliku `xgb_model_all_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ładowanie danych\n",
    "data_base_path = \"./data\"\n",
    "model_directory = \"./models\"\n",
    "train_data, valid_data = load_data(\"data\")\n",
    "\n",
    "# Inicjalizacja ekstraktora cech\n",
    "aggregator = FeatureAggregator(base_data_path=data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zakomentowane, bo za długo trwa trenowanie modelu\n",
    "\n",
    "# # Trenowanie i zapisanie modelu Gradient Boosting Classifier z walidacją krzyżową\n",
    "# trainer = Trainer(feature_aggregator=aggregator, model_dir=model_directory)\n",
    "# trainer.train(train_data, valid_data, feature_sets=\"all\", verbose=True, do_grid_search=True)\n",
    "# trainer.save_model(\"gb_gridcv_model_all_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.77394\n",
      "[1]\tvalidation_0-auc:0.78943\n",
      "[2]\tvalidation_0-auc:0.78873\n",
      "[3]\tvalidation_0-auc:0.79966\n",
      "[4]\tvalidation_0-auc:0.80249\n",
      "[5]\tvalidation_0-auc:0.80563\n",
      "[6]\tvalidation_0-auc:0.80865\n",
      "[7]\tvalidation_0-auc:0.81021\n",
      "[8]\tvalidation_0-auc:0.81112\n",
      "[9]\tvalidation_0-auc:0.81157\n",
      "[10]\tvalidation_0-auc:0.81219\n",
      "[11]\tvalidation_0-auc:0.81167\n",
      "[12]\tvalidation_0-auc:0.81270\n",
      "[13]\tvalidation_0-auc:0.81330\n",
      "[14]\tvalidation_0-auc:0.81366\n",
      "[15]\tvalidation_0-auc:0.81337\n",
      "[16]\tvalidation_0-auc:0.81350\n",
      "[17]\tvalidation_0-auc:0.81317\n",
      "[18]\tvalidation_0-auc:0.81338\n",
      "[19]\tvalidation_0-auc:0.81341\n",
      "[20]\tvalidation_0-auc:0.81369\n",
      "[21]\tvalidation_0-auc:0.81385\n",
      "[22]\tvalidation_0-auc:0.81451\n",
      "[23]\tvalidation_0-auc:0.81383\n",
      "[24]\tvalidation_0-auc:0.81474\n",
      "[25]\tvalidation_0-auc:0.81595\n",
      "[26]\tvalidation_0-auc:0.81662\n",
      "[27]\tvalidation_0-auc:0.81614\n",
      "[28]\tvalidation_0-auc:0.81651\n",
      "[29]\tvalidation_0-auc:0.81679\n",
      "[30]\tvalidation_0-auc:0.81701\n",
      "[31]\tvalidation_0-auc:0.81711\n",
      "[32]\tvalidation_0-auc:0.81692\n",
      "[33]\tvalidation_0-auc:0.81802\n",
      "[34]\tvalidation_0-auc:0.81762\n",
      "[35]\tvalidation_0-auc:0.81834\n",
      "[36]\tvalidation_0-auc:0.81894\n",
      "[37]\tvalidation_0-auc:0.81903\n",
      "[38]\tvalidation_0-auc:0.81918\n",
      "[39]\tvalidation_0-auc:0.81882\n",
      "[40]\tvalidation_0-auc:0.81883\n",
      "[41]\tvalidation_0-auc:0.81895\n",
      "[42]\tvalidation_0-auc:0.81886\n",
      "[43]\tvalidation_0-auc:0.81902\n",
      "[44]\tvalidation_0-auc:0.81976\n",
      "[45]\tvalidation_0-auc:0.82021\n",
      "[46]\tvalidation_0-auc:0.82049\n",
      "[47]\tvalidation_0-auc:0.82017\n",
      "[48]\tvalidation_0-auc:0.82017\n",
      "[49]\tvalidation_0-auc:0.81992\n",
      "[50]\tvalidation_0-auc:0.81983\n",
      "[51]\tvalidation_0-auc:0.81965\n",
      "[52]\tvalidation_0-auc:0.81986\n",
      "[53]\tvalidation_0-auc:0.81996\n",
      "[54]\tvalidation_0-auc:0.82017\n",
      "[55]\tvalidation_0-auc:0.82039\n",
      "[56]\tvalidation_0-auc:0.82100\n",
      "[57]\tvalidation_0-auc:0.82091\n",
      "[58]\tvalidation_0-auc:0.82070\n",
      "[59]\tvalidation_0-auc:0.82095\n",
      "[60]\tvalidation_0-auc:0.82115\n",
      "[61]\tvalidation_0-auc:0.82144\n",
      "[62]\tvalidation_0-auc:0.82139\n",
      "[63]\tvalidation_0-auc:0.82158\n",
      "[64]\tvalidation_0-auc:0.82200\n",
      "[65]\tvalidation_0-auc:0.82217\n",
      "[66]\tvalidation_0-auc:0.82214\n",
      "[67]\tvalidation_0-auc:0.82260\n",
      "[68]\tvalidation_0-auc:0.82263\n",
      "[69]\tvalidation_0-auc:0.82260\n",
      "[70]\tvalidation_0-auc:0.82293\n",
      "[71]\tvalidation_0-auc:0.82315\n",
      "[72]\tvalidation_0-auc:0.82351\n",
      "[73]\tvalidation_0-auc:0.82345\n",
      "[74]\tvalidation_0-auc:0.82329\n",
      "[75]\tvalidation_0-auc:0.82355\n",
      "[76]\tvalidation_0-auc:0.82400\n",
      "[77]\tvalidation_0-auc:0.82405\n",
      "[78]\tvalidation_0-auc:0.82424\n",
      "[79]\tvalidation_0-auc:0.82461\n",
      "[80]\tvalidation_0-auc:0.82500\n",
      "[81]\tvalidation_0-auc:0.82551\n",
      "[82]\tvalidation_0-auc:0.82540\n",
      "[83]\tvalidation_0-auc:0.82509\n",
      "[84]\tvalidation_0-auc:0.82535\n",
      "[85]\tvalidation_0-auc:0.82527\n",
      "[86]\tvalidation_0-auc:0.82528\n",
      "[87]\tvalidation_0-auc:0.82541\n",
      "[88]\tvalidation_0-auc:0.82580\n",
      "[89]\tvalidation_0-auc:0.82550\n",
      "[90]\tvalidation_0-auc:0.82516\n",
      "[91]\tvalidation_0-auc:0.82514\n",
      "[92]\tvalidation_0-auc:0.82500\n",
      "[93]\tvalidation_0-auc:0.82464\n",
      "[94]\tvalidation_0-auc:0.82473\n",
      "[95]\tvalidation_0-auc:0.82482\n",
      "[96]\tvalidation_0-auc:0.82481\n",
      "[97]\tvalidation_0-auc:0.82502\n",
      "[98]\tvalidation_0-auc:0.82535\n",
      "[Trainer] Training AUC: 0.8523\n",
      "[Trainer] Validation AUC: 0.8258\n",
      "[Trainer] Model saved to ./models\\xgb_model_all_features_2025_03_25__20_47_22.pkl\n"
     ]
    }
   ],
   "source": [
    "# Initialize the FeatureAggregator\n",
    "aggregator = FeatureAggregator(base_data_path=data_base_path)\n",
    "\n",
    "# Initialize and train the model on all features\n",
    "trainer = Trainer(feature_aggregator=aggregator, model_dir=model_directory)\n",
    "trainer.train(train_data, valid_data, feature_sets=\"all\", verbose=True, do_grid_search=False)\n",
    "trainer.save_model(\"xgb_model_all_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Ewaluacja modelu na zbiorze walidacyjnym\n",
    "\n",
    "W kolejnym kroku wczytujemy zapisany model i dane walidacyjne, a następnie używamy klasy `Evaluator` do oceny jakości modelu na zbiorze walidacyjnym. Ewaluacja obejmuje ekstrakcję cech, prognozowanie etykiet i obliczanie miary ROC AUC. Wynik jest wyświetlany na ekranie, co pozwala na ocenę skuteczności modelu w wykrywaniu halucynacji w odpowiedziach modelu językowego.\n",
    "\n",
    "Model osiąga wynik ROC AUC na poziomie 0.82 na zbiorze walidacyjnym, więc przy podobnym wyniku na zbiorze testowym otrzymałby maksymalną liczbę punktów w zadaniu konkursowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluator] Loaded model from ./models\\xgb_model_all_features_2025_03_25__20_47_22.pkl\n",
      "[Notebook] Validation AUC (All Features): 0.8258\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(feature_aggregator=aggregator, model_dir=model_directory)\n",
    "auc_score = evaluator.evaluate(valid_data, feature_sets=\"all\")\n",
    "print(f\"[Notebook] Validation AUC (All Features): {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hallucinations(sample):\n",
    "    \"\"\"\n",
    "    Predicts the probability that the given sample is a hallucination.\n",
    "    \n",
    "    :param sample: A dictionary representing a single data sample.\n",
    "    :return: Probability of the sample being a hallucination (float between 0 and 1).\n",
    "    \"\"\"\n",
    "    # Aggregate features using the FeatureAggregator\n",
    "    features = aggregator.aggregate_features(sample, feature_sets=\"all\")\n",
    "    \n",
    "    # Prepare the feature vector for prediction\n",
    "    X = [list(features.values())]\n",
    "    \n",
    "    # Get prediction probabilities from the trained XGBoost model\n",
    "    probs = trainer.model.predict_proba(X)\n",
    "    \n",
    "    # Assuming the first column corresponds to 'is_correct' = False (hallucination)\n",
    "    hallucination_prob = probs[0][0]\n",
    "    \n",
    "    return 1-hallucination_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float32(0.277938),\n",
       " np.float32(0.094361484),\n",
       " np.float32(0.109158814),\n",
       " np.float32(0.6077391),\n",
       " np.float32(0.15961957)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista prawdopodobieństw dla każdego przykładu w zestawie danych walidacyjnych\n",
    "hallucination_probs = [predict_hallucinations(sample) for sample in valid_data]\n",
    "hallucination_probs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Opcjonalne) Ocena podzbiorów wybranych cech\n",
    "\n",
    "W przypadku, gdy nie chcemy używać wszystkich cech, możemy wybrać podzbiór cech, które uważamy za najbardziej istotne dla naszego modelu. \n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ocena rozwiązania\n",
    "\n",
    "Uruchomienie poniższej komórki pozwoli sprawdza, ile punktów zdobyło nasze rozwiązanie na danych walidacyjnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets(dataset):\n",
    "    \"\"\"\n",
    "    Returns a 1D NumPy array of ground-truth labels (0 or 1) from the dataset.\n",
    "    'is_correct' is assumed to be boolean; True -> 1, False -> 0.\n",
    "    \"\"\"\n",
    "    ys = []\n",
    "    for entry in dataset:\n",
    "        ys.append(1 if entry.get('is_correct') else 0)\n",
    "    return np.array(ys, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(dataset, algorithm, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluates a hallucination-detection algorithm on the given dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The labeled dataset, where each entry is a dict with 'is_correct'.\n",
    "    algorithm : callable\n",
    "        A function that takes a sample dict and returns a hallucination probability.\n",
    "    verbose : bool\n",
    "        If True, print extra info per sample and summary at the end.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    roc_auc : float\n",
    "        The area under the ROC curve for the predictions.\n",
    "    balanced_acc : float\n",
    "        The balanced accuracy (average of recall for each class).\n",
    "    \"\"\"\n",
    "    predicted_ys = []\n",
    "\n",
    "    for i, entry in enumerate(dataset):\n",
    "        # Copy sample and remove label to form unlabeled input\n",
    "        sample_unlabeled = dict(entry)\n",
    "        sample_unlabeled.pop('is_correct', None)\n",
    "\n",
    "        try:\n",
    "            # 1. Predict probability for single sample\n",
    "            pred_prob = algorithm(sample_unlabeled)\n",
    "            predicted_ys.append(pred_prob)\n",
    "\n",
    "        except Exception as e:\n",
    "            # If there's an error, score zero or any default you prefer\n",
    "            predicted_ys.append(0.5)\n",
    "            if verbose:\n",
    "                print(f\"Sample {i} => Error: {e}\")\n",
    "\n",
    "    # Convert predictions and ground truth to numpy arrays\n",
    "    predicted_ys = np.array(predicted_ys, dtype=np.float32)\n",
    "    ys = extract_targets(dataset)\n",
    "\n",
    "    # Compute metrics\n",
    "    roc_auc = roc_auc_score(ys, predicted_ys)\n",
    "    balanced_acc = balanced_accuracy_score(ys, (predicted_ys > 0.5).astype(int))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nNumber of samples: {len(dataset)}\")\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "\n",
    "    return roc_auc, balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of samples: 990\n",
      "ROC AUC: 0.8258\n",
      "Balanced Accuracy: 0.7129\n"
     ]
    }
   ],
   "source": [
    "roc_auc = evaluate_algorithm(valid, predict_hallucinations, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podczas sprawdzania model zostanie zapisany jako `your_model.pkl` i oceniony na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# nie mam dostępu do danych testowych, więc nie mogę przeprowadzić ostatecznej ewaluacji\u001b[39;00m\n\u001b[32m      2\u001b[39m test_path = os.path.join(\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtest.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     test = json.load(f)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bart_local\\anaconda3\\envs\\2_wykrywanie_halucynacji\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data\\\\test.json'"
     ]
    }
   ],
   "source": [
    "# nie mam dostępu do danych testowych, więc nie mogę przeprowadzić ostatecznej ewaluacji\n",
    "test_path = os.path.join('data', 'test.json')\n",
    "\n",
    "with open(test_path, 'r') as f:\n",
    "    test = json.load(f)\n",
    "# evaluate\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    roc_auc, balanced_acc = evaluate_algorithm(test, predict_hallucinations, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2_wykrywanie_halucynacji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
