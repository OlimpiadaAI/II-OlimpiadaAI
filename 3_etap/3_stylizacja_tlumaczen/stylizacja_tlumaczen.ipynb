{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b861af3",
   "metadata": {},
   "source": [
    "# Stylizacja tłumaczeń\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/54530431746_73942f964a_n.jpg\" alt=\"Embedded Photo\" width=\"500\">\n",
    "\n",
    "*Obraz wygenerowany przy użyciu modelu DALL-E.*\n",
    "\n",
    "## Wstęp\n",
    "\n",
    "Aiga poszła do księgarni i kupiła książkę o deep learningu. Była bardzo podekscytowana — w końcu porządnie odświeży swoją wiedzę przed finałem Olimpiady Sztucznej Inteligencji!\n",
    "\n",
    "Wróciła do domu, usiadła wygodnie z kubkiem herbaty i zaczęła czytać pierwsze zdania:\n",
    "\n",
    "> *\"Zanurzenia zdaniowe otrzymane z modeli typu enkoder są zwykle lepsze od tylko dekoderowych.\"*\n",
    "> \n",
    "> \n",
    "> *\"Uczenie ze wzmocnieniem z ludzkiego nadzoru typowo wykonuje się przez optymalizowanie polityki proksymalnej do modelu nagród.\"*\n",
    "\n",
    "\n",
    "— Co...? — mruknęła Aiga, patrząc zdezorientowana na kartki.\n",
    "\n",
    "Zadzwoniła do kolegi, który od razu jej wyjaśnił:\n",
    "\n",
    "> *\"Sentence embeddingi są lepsze, jeśli wyciągniesz je z encoderów niż z modeli decoder-only.\"*\n",
    "> \n",
    "> \n",
    "> *\"RLHF typowo używa PPO, żeby optymalizować politykę względem reward modelu.\"*\n",
    "\n",
    "\n",
    "Aiga odetchnęła z ulgą. Teraz wszystko stało się *trochę* jaśniejsze.\n",
    "\n",
    "Ulga była jednak tylko chwilowa — nadal nie mogła pogodzić się ze stylem tłumaczeń w książce. Postanowiła więc wytrenować własny model tłumaczenia z angielskiego na polski — taki, który będzie tłumaczył terminy związane z AI tak, jak jej się podoba!\n",
    "\n",
    "## Zadanie\n",
    "\n",
    "W tym zadaniu Twoim celem jest dostosowanie tłumaczeń generowanych przez istniejący model tłumaczenia maszynowego do specyficznego stylu – zachowania oryginalnego brzmienia niektórych terminów branżowych z dziedziny uczenia maszynowego.\n",
    "\n",
    "Model bazowy, którego używamy, to [MarianMT](https://huggingface.co/docs/transformers/en/model_doc/marian) – model tłumaczący z języka angielskiego na polski, oparty na architekturze enkoder-dekoder. Domyślnie model ten tłumaczy wszystkie słowa, również terminologię specjalistyczną. Twoim zadaniem będziezaimplementowanie funkcji przetwarzającej dane wejściowe i dotrenowanie tego modelu, aby terminologia specjalistyczna nie była tłumaczona na język polski.\n",
    "\n",
    "## Dane\n",
    "\n",
    "Dostępne dla Ciebie w tym zadaniu dane to:\n",
    "\n",
    "* **Zbiór treningowy** - zawiera angielskie zdania, ich tłumaczenia na język polski oraz listę słów kluczowych, które powinny pozostać nieprzetłumaczone.\n",
    "\n",
    "* **Zbiór walidacyjny** - służy do oceny skuteczności Twojego podejścia podczas trenowania modelu; ma ten sam format co zbiór treningowy.\n",
    "\n",
    "Przykłady mają format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"keywords\": [\"explainable AI\"],\n",
    "    \"en\": \"Developing explainable AI tools is crucial for trust in automated systems.\",\n",
    "    \"pl\": \"Rozwijanie narzędzi explainable AI ma kluczowe znaczenie dla zaufania do zautomatyzowanych systemów.\",\n",
    "}\n",
    "```\n",
    "\n",
    "Zbiór testowy, na których finalnie będzie oceniane Twoje rozwiązanie **nie będzie** zawierał polskich tłumaczeń zdań.\n",
    "\n",
    "## Kryterium Oceny\n",
    "\n",
    "Twoje rozwiązanie zostanie ocenione na ukrytych danych testowych, za pomocą metryki **BLEU**. Zbiór testowy jest podobny do zbioru walidacyjnego.\n",
    "\n",
    "BLEU mierzy zgodność *n-gramów* (spójnych ciągów *n* sąsiadujących słów) pomiędzy Twoim tłumaczeniem a pojedynczym zdaniem referencjnym – im większa zgodność, tym wyższy wynik.\n",
    "\n",
    "## Ograniczenia\n",
    "\n",
    "* Twoje rozwiązanie będzie testowane na Platformie Konkursowej bez dostępu do internetu oraz w środowisku **z GPU**.\n",
    "* Trening oraz ewaluacja Twojego rozwiązania na Platformie Konkursowej nie może trwać dłużej niż **10 minut**.\n",
    "* Lista dopuszczalnych bibliotek: `torch`, `pandas`, `numpy`, `nltk`, `transformers`, `datasets`, `matplotlib`.\n",
    "\n",
    "## Pliki Zgłoszeniowe\n",
    "\n",
    "* Ten notebook uzupełniony o Twoje rozwiązanie.\n",
    "\n",
    "## Ewaluacja\n",
    "\n",
    "Pamiętaj, że podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`.\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy 0 a 100 punktów. Jeśli Twój model osiągnie metykę BLEU niższą niż **0.82** na (tajnym) zbiorze testowym na Platformie Konkursowej, Twoje rozwiązanie otrzyma 0 punktów. Jeśli osiągnie wynik wyższy niż **0.86**, zdobędziesz maksymalną liczbę punktów. Pomiędzy tymi progami, Twój wynik będzie skalował się liniowo i zostanie zaokrąglony do liczby całkowitej.\n",
    "\n",
    "$$\n",
    "\\mathrm{punkty} = \n",
    "\\begin{cases}\n",
    "100 & \\text{jeśli } \\mathrm{BLEU} > 0.86 \\\\\n",
    "0 & \\text{jeśli } \\mathrm{BLEU} < 0.82 \\\\\n",
    "100 \\cdot \\dfrac{\\mathrm{BLEU} - 0.82}{0.86 - 0.82} & \\text{w przeciwnym razie}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Kod Startowy\n",
    "W tej sekcji inicjalizujemy środowisko poprzez zaimportowanie potrzebnych bibliotek i funkcji. Przygotowany kod ułatwi Tobie efektywne operowanie na danych i budowanie właściwego rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba578e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "\n",
    "FINAL_EVALUATION_MODE = False  # W czasie sprawdzania Twojego rozwiązania, zmienimy tą wartość na True\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "\n",
    "from transformers import set_seed\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275b31b",
   "metadata": {},
   "source": [
    "## Ładowanie Danych\n",
    "W tej części zadania wczytamy dane treningowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "def load_dataset(json_path: str) -> Dataset:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    dataset = []\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        dataset.append(\n",
    "            {\n",
    "                \"en\": item[\"translation\"][\"en\"],\n",
    "                \"pl\": item[\"translation\"][\"pl\"],\n",
    "                \"keywords\": \",\".join(item.get(\"keywords\", []))\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return Dataset.from_list(dataset)\n",
    "    \n",
    "\n",
    "train_dataset = load_dataset(\"train_dataset.jsonl\")\n",
    "print(f\"Loaded {len(train_dataset)} training examples.\")\n",
    "\n",
    "val_dataset = load_dataset(\"valid_dataset.jsonl\")\n",
    "print(f\"Loaded {len(val_dataset)} validation examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373d517",
   "metadata": {},
   "source": [
    "## Kod z kryterium oceniającym\n",
    "\n",
    "Kod zbliżony do poniższego, będzie używany do oceny rozwiązania na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d47497",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, process_example_fn, batch_size=64, device=\"cuda\", verbose=True):\n",
    "    \"\"\"\n",
    "    Funkcja ocenia model na podstawie metryki BLEU.\n",
    "    Argumenty:\n",
    "        model: Model do oceny.\n",
    "        tokenizer: Tokenizer używany do przetwarzania tekstu.\n",
    "        dataset: Zbiór danych do oceny.\n",
    "        process_example_fn: Funkcja przetwarzająca przykłady: \n",
    "            (en_sentence: str, keywords: List[str]) -> (input_sentence: str).\n",
    "        verbose: Czy wyświetlać szczegóły dla pierwszych kilku przykładów.\n",
    "    Wynik:\n",
    "        float: Średni wynik BLEU dla zbioru danych.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    bleu_scores = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        orig_sentences = batch[\"en\"]\n",
    "        reference_translations = batch[\"pl\"]\n",
    "        keywords = batch[\"keywords\"]\n",
    "\n",
    "        model_inputs = [\n",
    "            process_example_fn(orig, kws) \n",
    "            for orig, kws in zip(orig_sentences, keywords)\n",
    "        ]\n",
    "\n",
    "        inputs = tokenizer(model_inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "        hypotheses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        for i, (ref, hyp) in enumerate(zip(reference_translations, hypotheses)):\n",
    "            score = sentence_bleu([ref], hyp)\n",
    "            bleu_scores.append(score)\n",
    "\n",
    "            if verbose and batch_idx * batch_size + i < 5:\n",
    "                print(f\"Example {batch_idx * batch_size + i}:\")\n",
    "                print(f\"Original: {orig_sentences[i]}\")\n",
    "                print(f\"Processed: {model_inputs[i]}\")\n",
    "                print(f\"Reference: {ref}\")\n",
    "                print(f\"Hypothesis: {hyp}\")\n",
    "                print(f\"BLEU score: {score:.4f}\")\n",
    "                print(\"-\" * 10)\n",
    "\n",
    "    bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def compute_score(bleu_score: float) -> float:\n",
    "    \"\"\"\n",
    "    Oblicza wynik punktowy na podstawie wartości metryki BLEU.\n",
    "    \"\"\"\n",
    "    lower_bound = 0.82\n",
    "    upper_bound = 0.86\n",
    "\n",
    "    if bleu_score <= lower_bound:\n",
    "        return 0\n",
    "    elif lower_bound < bleu_score < upper_bound:\n",
    "        return int(round(100 * (bleu_score - lower_bound) / (upper_bound - lower_bound)))\n",
    "    else:\n",
    "        return 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da274a1",
   "metadata": {},
   "source": [
    "## Twoje Rozwiązanie\n",
    "\n",
    "\n",
    "W tej sekcji należy zaimplementować funkcję `process_example`, wytrenować model i zapisać go jako zmienną o nazwie `my_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(en: str, keywords: str) -> str:\n",
    "    \"\"\"\n",
    "    Funkcja, która przetwarza przykłady ewaluacyjne na tekst\n",
    "    wejściowy do modelu.\n",
    "    W czasie treningu możesz, ale nie musisz korzystać z tej funkcji.\n",
    "    Argumenty:\n",
    "        en: Tekst w języku angielskim.\n",
    "        keywords: Słowa kluczowe oddzielone przecinkami.\n",
    "    \"\"\"\n",
    "    # TODO: Zaimplementuj funkcję\n",
    "\n",
    "    return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Trening modelu.\n",
    "# Nie wolno zmieniać tokenizatora.\n",
    "# Pamiętaj, że masz do dyspozycji model oraz tokenizator \"gsarti/opus-mt-tc-en-pl\".\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gsarti/opus-mt-tc-en-pl\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"gsarti/opus-mt-tc-en-pl\")\n",
    "\n",
    "# TODO: ...\n",
    "\n",
    "my_model = model  # TODO: Na zmienną \"my_model\" musisz przypisać swój finalny model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0ab1b",
   "metadata": {},
   "source": [
    "## Ewaluacja\n",
    "\n",
    "Uruchomienie poniższej komórki pozwoli sprawdzić, ile punktów zdobyłoby Twoje rozwiązanie na danych walidacyjnych. Przed wysłaniem upewnij się, że cały notebook wykonuje się od początku do końca bez błędów po ustawieniu flagi *FINAL_EVALUATION_MODE = True* i bez konieczności ingerencji użytkownika po wybraniu opcji \"Run All\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gsarti/opus-mt-tc-en-pl\")\n",
    "    bleu_score = evaluate_model(\n",
    "        model=my_model, \n",
    "        tokenizer=tokenizer, \n",
    "        dataset=val_dataset,\n",
    "        process_example_fn=process_example,\n",
    "    )\n",
    "    print(f\"Wynik BLEU zbiorze walidacyjnym: {bleu_score:.4f}\")\n",
    "\n",
    "    score = compute_score(bleu_score)\n",
    "    print(f\"Punkty na zbiorze walidacyjnym: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729fa1e",
   "metadata": {},
   "source": [
    "Podczas sprawdzania model zostanie zapisany do pliku `your_model.pkl`, a funkcja przetwarzająca do pliku `your_function.pkl` i zostaną one ocenione na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if FINAL_EVALUATION_MODE:\n",
    "    import cloudpickle\n",
    "\n",
    "    OUTPUT_PATH = \"file_output\"\n",
    "\n",
    "    FUNCTION_OUTPUT_PATH = os.path.join(OUTPUT_PATH, \"your_function.pkl\")\n",
    "    MODEL_OUTPUT_PATH = os.path.join(OUTPUT_PATH, \"your_model.pkl\")\n",
    "\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "    with open(FUNCTION_OUTPUT_PATH, \"wb\") as f:\n",
    "        cloudpickle.dump(process_example, f)\n",
    "\n",
    "    with open(MODEL_OUTPUT_PATH, \"wb\") as f:\n",
    "        cloudpickle.dump(my_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
