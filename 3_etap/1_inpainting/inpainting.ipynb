{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504bcaec",
   "metadata": {
    "id": "504bcaec"
   },
   "source": [
    "# Uzupełnianie przy użyciu sieci INR (ang. *Implicit Neural Representations*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d868442",
   "metadata": {
    "id": "8d868442"
   },
   "source": [
    "## Wstęp\n",
    "\n",
    "W ostatnich latach sztuczna inteligencja coraz skuteczniej radzi sobie z przetwarzaniem obrazów i filmów – nie tylko rozpoznaje obiekty, ale także potrafi **uzupełniać brakujące fragmenty obrazu**, **kompresować dane** czy nawet **generować nowe klatki filmu**.\n",
    "\n",
    "Wyobraź sobie, że masz do dyspozycji film, w którym część obrazu – np. prostokątny fragment w każdej klatce – została zasłonięta, tak jakby ktoś wyciął tę część nożyczkami lub zakrył ją taśmą. Twoim zadaniem jest **odtworzenie brakujących fragmentów tylko na podstawie współrzędnych piksela i czasu**.\n",
    "\n",
    "W tym zadaniu skupimy się na bardziej zaawansowanym problemie: stworzeniu modelu całego filmu jako funkcji, która dla dowolnych współrzędnych piksela i numeru klatki zwraca jego kolor i przynależność do obiektu, wykorzystując techniki uzupełniania (ang. **inpaintingu**) oraz **sieci INR**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8876dad",
   "metadata": {
    "id": "f8876dad"
   },
   "source": [
    "### Czym jest inpainting?\n",
    "\n",
    "**Inpainting** to technika stosowana w przetwarzaniu obrazów i filmów, która polega na **uzupełnianiu brakujących fragmentów** obrazu lub wideo w sposób jak najbardziej realistyczny. Nazwa pochodzi z języka angielskiego i oznacza dosłownie „domalowywanie”. Wykorzystuje się ją m.in. do usuwania obiektów ze zdjęć, rekonstrukcji zniszczonych fotografii czy naprawy uszkodzonych klatek filmowych.\n",
    "\n",
    "Przykładowe użycie inpaintingu:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JV9KutOgT1a",
   "metadata": {
    "id": "1JV9KutOgT1a"
   },
   "source": [
    "![](https://live.staticflickr.com/65535/54554097818_7395667a21.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63JpcONZgZZp",
   "metadata": {
    "id": "63JpcONZgZZp"
   },
   "source": [
    "W naszym zadaniu będziemy pracować nad bardziej zaawansowanym przypadkiem: **inpaintingiem dynamicznym**, czyli uzupełnianiem **brakujących fragmentów filmu w czasie**. Braki występują tylko w wybranym obszarze (np. środek klatki), który został wycięty we wszystkich klatkach filmu. Co ważne model nie musi generować całej klatki – jego zadaniem jest **tylko przewidzenie zawartości brakującego regionu**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9b04c",
   "metadata": {
    "id": "afe9b04c"
   },
   "source": [
    "### Czym są sieci INR (Implicit Neural Representations)?\n",
    "\n",
    "**INR (Implicit Neural Representations)** to nowoczesna technika reprezentowania danych przy użyciu sieci neuronowych. Tradycyjnie obrazy czy filmy przechowujemy jako macierze (siatki) wartości pikseli – każda komórka takiej macierzy to kolor piksela. Jednak INRy działają inaczej: zamiast zapisywać wartości pikseli bezpośrednio, sieć uczy się **funkcji**, która dla podanych współrzędnych przestrzennych (x, y) i – w naszym przypadku – także czasowej (t) zwraca przewidywane wartości.\n",
    "\n",
    "Czyli zamiast przechowywać cały film, uczymy sieć, która działa jak „wirtualny projektor”:\n",
    "\n",
    "$$\n",
    "f(x, y, t) \\rightarrow (R, G, B)\n",
    "$$\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "* $x, y$ – współrzędne piksela,\n",
    "* $t$ – numer klatki filmu (czas),\n",
    "* $R, G, B$ – kolor piksela w tej lokalizacji,\n",
    "\n",
    "Taka reprezentacja ma wiele zalet:\n",
    "\n",
    "* pozwala generować obraz w dowolnej rozdzielczości (nie ogranicza nas konkretna siatka pikseli),\n",
    "* umożliwia interpolację pomiędzy klatkami (np. tworzenie płynnego ruchu),\n",
    "* działa dobrze w zadaniach takich jak kompresja, super-rozdzielczość czy właśnie inpainting.\n",
    "\n",
    "Dzieje się tak, ponieważ sieć nie ogranicza nas do wyboru wartości całkowitych, możemy wybierać dowolne wartości pośrednie np. (1.5, 44.5, 13.25).\n",
    "\n",
    "Przykładowe wykorzystanie sieci INR pokazano na grafice:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uj2-o6p1gdmj",
   "metadata": {
    "id": "Uj2-o6p1gdmj"
   },
   "source": [
    "![](https://live.staticflickr.com/65535/54555506837_48f1f8e589_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2R1-S2rgcpu",
   "metadata": {
    "id": "p2R1-S2rgcpu"
   },
   "source": [
    "W naszym zadaniu model INR będzie reprezentował tylko **brakujący fragment filmu**, czyli obszar, którego nie znamy. Sieć musi nauczyć się nie tylko tego, **jak wygląda obraz w czasie**, ale także tego, **co się znajduje w niewidocznym obszarze**. To oznacza, że musi rozpoznać kontekst sceny – na podstawie otoczenia – i na tej podstawie realistycznie odtworzyć brakujące dane (zadanie *inpaintingu*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6693b",
   "metadata": {
    "id": "e9e6693b"
   },
   "source": [
    "### Czym jest maska segmentacji?\n",
    "\n",
    "Druga część problemu dotyczy **segmentacji obrazu**. Segmentacja to proces rozpoznawania, które piksele należą do obiektu (np. postaci, samochodu, drzewa), a które do tła. Można to zapisać w postaci **maski** – obrazu (w naszym przypadku dla uproszczenia użyjemy maski binarnej), w którym każdy piksel ma wartość 0 (nie należy do obiektu) lub 1 (należy do obiektu). Dzięki temu można np. oddzielić postać od tła, co jest użyteczne w wielu zastosowaniach – od autonomicznych pojazdów po edycję wideo.\n",
    "\n",
    "Na grafice przedstawiono przykładową maskę binarną:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3nPxs5QHgpNu",
   "metadata": {
    "id": "3nPxs5QHgpNu"
   },
   "source": [
    "![](https://live.staticflickr.com/65535/54554043929_118739686d_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zTg1_g7fgmjQ",
   "metadata": {
    "id": "zTg1_g7fgmjQ"
   },
   "source": [
    "Projektowany model będzie miał za zadanie przewidzieć **zarówno kolor piksela**, jak i jego **klasę (obiekt/tło)** – i to tylko dla brakującego obszaru. Dlatego też powyższą funkcję można rozszerzyć do postaci:\n",
    "\n",
    "$$\n",
    "f(x, y, t) \\rightarrow (R, G, B, m)\n",
    "$$\n",
    "\n",
    "Gdzie:\n",
    "\n",
    "* $x, y$ – współrzędne piksela,\n",
    "* $t$ – numer klatki filmu (czas),\n",
    "* $R, G, B$ – kolor piksela w tej lokalizacji,\n",
    "* $m \\in \\{0,1\\}$ – wartość maski segmentacyjnej."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615cb12",
   "metadata": {
    "id": "5615cb12"
   },
   "source": [
    "## Zadanie\n",
    "\n",
    "Twoim zadaniem jest zbudowanie sieci typu **INR** (Implicit Neural Representation), która przyjmuje jako wejście trzy liczby:\n",
    "\n",
    "* **x** – współrzędna pozioma piksela,\n",
    "* **y** – współrzędna pionowa piksela,\n",
    "* **t** – numer klatki filmu.\n",
    "\n",
    "> *Uwaga: Wartości wejściowe nie muszą być liczbami całkowitymi – sieć powinna działać również dla współrzędnych ciągłych, co umożliwia interpolację.*\n",
    "\n",
    "Na wyjściu sieć powinna przewidywać:\n",
    "\n",
    "* **wartości RGB** – czyli kolor piksela w danym momencie filmu,\n",
    "* **wartość maski segmentacyjnej** – wartość 0 lub 1, która oznacza przynależność piksela do obiektu (0 – tło, 1 – obiekt).\n",
    "\n",
    "### Dane\n",
    "\n",
    "W zadaniu udostępniamy dwa zbiory danych:\n",
    "\n",
    "* **Zbiór treningowy** – zawiera współrzędne punktów oraz odpowiadające im wartości RGB i maski, przeznaczony do nauki modelu,\n",
    "* **Zbiór walidacyjny** – służy do oceny jakości predykcji modelu na niewidocznych wcześniej danych.\n",
    "\n",
    "Dla wygody przygotowaliśmy gotowy **dataloader**, który dostarcza dane w postaci rekordów zawierających:\n",
    "\n",
    "* współrzędne piksela: `(x, y, t)`,\n",
    "* wartość RGB oryginalnego obrazu,\n",
    "* wartość maski segmentacyjnej z zbioru `{0, 1}`.\n",
    "\n",
    "Obrazy wejściowe mają rozdzielczość **256 × 256 pikseli**. Zbiory zawierają odpowiednio:\n",
    "\n",
    "* **zbiór treningowy** – 59 obrazów,\n",
    "* **zbiór walidacyjny** – 10 obrazów,\n",
    "* **zbiór testowy** – 10 obrazów.\n",
    "\n",
    "W zbiorze walidacyjnym udostępnione są wyłącznie współrzędne pikseli należących do **brakującego obszaru** – to właśnie ten region (o rozmiarze **64 × 64**) ma zostać *zrekonstruowany* przez model. Oznacza to, że model powinien nauczyć się uzupełniać brakujące fragmenty na podstawie wcześniej poznanych danych.\n",
    "\n",
    "Twoje rozwiązanie zostanie ostatecznie przetestowane na Platformie Konkursowej na ukrytym zestawie danych testowych, który nie różni się znacząco od zbioru walidacyjnego pod względem rozkładu danych.\n",
    "\n",
    "### Kryterium Oceny\n",
    "\n",
    "Jak możesz się spodziewać, w ewaluacji będziemy oceniać dwa kluczowe aspekty Twojego rozwiązania:\n",
    "\n",
    "- **Jakość rekonstrukcji obszaru brakującego obrazu** - jak dobrej jakości jest zwrócony obszar pikseli RGB, oraz ich spójność, ocenione zostanie to za pomocą metryki *PSNR*,\n",
    "- **Dokładność predykcji binarnej maski** - jak dobrze udało się przewidzieć wartości maski w brakującym obszarze, ocenione zostanie to za pomocą dokładności klasyfikacji *acc*.\n",
    "\n",
    "Przy definicji: **PSNR** (*peak signal-to-noise ratio*) - popluarna metryka jakości rekonstrukcji (np. obrazu).\n",
    "\n",
    "Wartości *PSNR* i *acc* powstają z uśrednienia poszczególnych wartości na całym zbiorze testowym.\n",
    "\n",
    "Finalny wynik definiujemy jako średnią ważoną tych dwóch aspektów:\n",
    "\n",
    "$$ score = 0.7 \\cdot P_{PSNR} \\cdot 10 + 0.3 \\cdot P_{acc} \\cdot 10 $$\n",
    "\n",
    "gdzie $P_{PSNR}$ to punkty przyznane za jakościowy wynik (*wartość metryki PSNR*) rozwiązania wynikający z progów:\n",
    "\n",
    "$$ P_{PSNR} = \\begin{cases}\n",
    "0 & \\text{jeżeli } PSNR < 14 \\\\\n",
    "2 & \\text{jeżeli } 14 <= PSNR < 17.1 \\\\\n",
    "\\frac{5}{4} \\cdot PSNR - 19.375 & \\text{jeżeli } 17.1 <= PSNR < 23.5 \\\\\n",
    "10 & \\text{jeżeli } 23.5 <= PSNR\n",
    "\\end{cases} $$\n",
    "\n",
    "a $P_{acc}$ to punkty przyznane za dokładność maski (*wartość metryki acc*) wynikający z progów:\n",
    "\n",
    "\n",
    "$$ P_{acc} = \\begin{cases}\n",
    "0 & \\text{jeżeli } acc < 0.83 \\\\\n",
    "\\frac{20}{3} \\cdot acc - \\frac{83}{15} & \\text{jeżeli } 0.83 <= acc < 0.98 \\\\\n",
    "10 & \\text{jeżeli } 0.98 <= acc\n",
    "\\end{cases} $$\n",
    "\n",
    "Ta formuła wyraża, że aby uzyskać punkty, Twoje rozwiązanie musi osiągnąć minimalny wynik *PSNR* równy $14$ lub minimalny wynik *acc* równy $0.83$, a maksymalna liczba punktów ($100$) jest przyznawana za rozwiązania o wyniku wartości *PSNR* od $23.5$ (włącznie) i wartości *acc* od $0.98$ (włącznie).\n",
    "\n",
    "> **Uwaga: Modele oparte na standardowych aktywacjach typu ReLU mogą nie wystarczyć.** \\\n",
    "> *Prosta sieć z aktywacjami ReLU osiąga jedynie około 5 punktów jakości rekonstrukcji (PSNR) na zbiorze walidacyjnym – aby uzyskać wyraźnie lepsze rezultaty, rozważ użycie aktywacji sinusoidalnych zgodnie z podejściem SIREN (Implicit Neural Representations with Periodic Activation Functions), https://arxiv.org/abs/2006.09661.*\n",
    "\n",
    "Szczegóły implementacji tej formuły znajdziesz w funkcji `grade` w kodzie zadania.\n",
    "\n",
    "## Ograniczenia\n",
    "\n",
    "* Twoje rozwiazanie będzie testowane na Platformie Konkursowej.\n",
    "* Model **nie** może korzystać z innych zbiorów danych oraz z pretrenowanych wag na innych zbiorach danych.\n",
    "* Model może być uczony **maksymalnie** 6 minut z użyciem GPU.\n",
    "\n",
    "## Pliki zgłoszeniowe\n",
    "\n",
    "Ten notebook uzupełniony o Twoje rozwiązanie (patrz klasa `YourSolution`).\n",
    "\n",
    "## Ewaluacja\n",
    "\n",
    "Pamiętaj, że podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`.\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy 0 a 100 punktów. Liczba punktów, którą zdobędziesz, będzie wyliczona na (tajnym) zbiorze testowym na Platformie Konkursowej na podstawie wyżej wspomnianego wzoru, zaokrąglona do liczby całkowitej. Jeśli Twoje rozwiązanie nie będzie spełniało powyższych kryteriów lub nie będzie wykonywać się prawidłowo, otrzymasz za zadanie 0 punktów.\n",
    "\n",
    "Poniższa grafika ilustruje przykładowy proces inpaintingu z wykorzystaniem sieci INR:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kG8pjtD8hOyK",
   "metadata": {
    "id": "kG8pjtD8hOyK"
   },
   "source": [
    "![](https://live.staticflickr.com/65535/54554043974_246b208130_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619479d",
   "metadata": {
    "id": "3619479d"
   },
   "source": [
    "## Kod Startowy\n",
    "\n",
    "W tej sekcji inicjalizujemy środowisko poprzez zaimportowanie potrzebnych bibliotek i funkcji. Przygotowany kod ułatwi Tobie efektywne operowanie na danych i budowanie właściwego rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872650e",
   "metadata": {
    "id": "e872650e"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# W czasie sprawdzania Twojego rozwiązania, wartość flagi FINAL_EVALUATION_MODE zostanie zmieniona na True\n",
    "FINAL_EVALUATION_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97448cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "# Ustawienie ziarna generatora liczb pseudolosowych w celu zapewnienia deterministycznych wyników.\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f40292",
   "metadata": {},
   "source": [
    "### Ładowanie Danych\n",
    "\n",
    "Za pomocą poniższego kodu wczytujemy dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1615d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Klasa ImageDataset reprezentuje zbiór danych.\n",
    "    Obsługuje obrazy i odpowiadające im maski, a także generuje współrzędne\n",
    "    dla każdego piksela obrazu wraz z wartością czasową (t_value).\n",
    "    \"\"\"\n",
    "\n",
    "    # Liczba klatek w filmie\n",
    "    VIDEO_LENGTH = 79\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dir: Path,\n",
    "        mask_dir: Path,\n",
    "        frame_names: list[str],\n",
    "        mode: str,\n",
    "        json_path: str | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicjalizuje obiekt klasy ImageDataset.\n",
    "\n",
    "        Parametry:\n",
    "        ----------\n",
    "        img_dir : Path\n",
    "            Ścieżka do katalogu z obrazami.\n",
    "        mask_dir : str\n",
    "            Ścieżka do katalogu z maskami.\n",
    "        frame_names : list[str]\n",
    "            Lista nazw plików obrazów (bez ścieżek).\n",
    "        mode : str\n",
    "            Tryb działania zbioru danych (\"train\" lub \"val\").\n",
    "        json_path : str | None\n",
    "            Ścieżka do pliku JSON z współrzędnymi prostokąta (domyślnie None).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.frame_names = frame_names\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode not in [\"train\", \"val\"]:\n",
    "            raise ValueError(\"Niepoprawny tryb. Użyj 'train' lub 'val'.\")\n",
    "\n",
    "        if mode == \"val\" and json_path is None:\n",
    "            raise ValueError(\n",
    "                \"W trybie 'val' musisz podać ścieżkę do pliku JSON z współrzędnymi prostokąta.\"\n",
    "            )\n",
    "\n",
    "        if self.mode == \"val\":\n",
    "            # Wczytanie współrzędnych brakującego prostokąta z pliku JSON\n",
    "            with open(json_path, \"r\") as f:\n",
    "                self.rect_coords = json.load(f)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Zwraca liczbę ramek w zbiorze danych.\n",
    "\n",
    "        Returns:\n",
    "            Liczba ramek: int\n",
    "        \"\"\"\n",
    "        return len(self.frame_names)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Zwraca dane dla podanego indeksu.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Indeks klatki.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Słownik zawierający w zależności od trybu; (x, y, t),\n",
    "            obrazy, maski i inne dane.\n",
    "        \"\"\"\n",
    "        # Pobranie nazwy ramki\n",
    "        frame_name = self.frame_names[idx]\n",
    "        img_name = frame_name\n",
    "        mask_name = frame_name.replace(\".jpg\", \".png\")\n",
    "\n",
    "        # Wczytanie obrazu i maski\n",
    "        img = cv2.imread(os.path.join(self.img_dir, img_name))\n",
    "        mask = cv2.imread(os.path.join(self.mask_dir, mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            # Wczytanie współrzędnych prostokąta i normalizacja\n",
    "            h, w = img.shape[:2]\n",
    "            img = img.astype(\"float32\") / 255.0\n",
    "            mask = mask.astype(\"float32\") / 255.0\n",
    "\n",
    "            # Generowanie siatki współrzędnych (x, y)\n",
    "            x = torch.linspace(-1, 1, w)\n",
    "            y = torch.linspace(-1, 1, h)\n",
    "            grid_y, grid_x = torch.meshgrid(y, x, indexing=\"ij\")\n",
    "            coords = torch.stack([grid_x, grid_y], dim=-1).reshape(h * w, 2)\n",
    "\n",
    "            # Obliczanie wartości czasowej (t_value)\n",
    "            t_value = int(os.path.splitext(frame_name)[0])\n",
    "            t_value = (t_value * 2) / (self.VIDEO_LENGTH - 1)\n",
    "            t = torch.full((coords.shape[0], 1), t_value, dtype=torch.float32)\n",
    "            coords = torch.cat([coords, t], dim=-1)\n",
    "\n",
    "            # Spłaszczenie obrazu i maski do formatu per-pikselowego\n",
    "            img = torch.tensor(img).view(-1, 3)\n",
    "            mask = torch.tensor(mask).view(-1, 1)\n",
    "\n",
    "            output = {\n",
    "                \"coordinates\": coords,  # (x, y, t)\n",
    "                \"rgb\": img,  # Wartości RGB prostokąta\n",
    "                \"mask\": mask,  # Maska prostokąta\n",
    "            }\n",
    "\n",
    "        elif self.mode == \"val\":\n",
    "            # Pobranie współrzędnych prostokąta\n",
    "            x1, y1, x2, y2 = (\n",
    "                self.rect_coords[frame_name][\"x1\"],\n",
    "                self.rect_coords[frame_name][\"y1\"],\n",
    "                self.rect_coords[frame_name][\"x2\"],\n",
    "                self.rect_coords[frame_name][\"y2\"],\n",
    "            )\n",
    "\n",
    "            # Wymiary obrazu i maski\n",
    "            h, w = x2 - x1, y2 - y1\n",
    "\n",
    "            # Normalizacja obrazu i maski\n",
    "            img = img.astype(\"float32\") / 255.0\n",
    "            mask = mask.astype(\"float32\") / 255.0\n",
    "\n",
    "            # Generowanie siatki współrzędnych (x, y) w obrębie prostokąta\n",
    "            x = torch.linspace(x1 / img.shape[1] * 2 - 1, x2 / img.shape[1] * 2 - 1, w)\n",
    "            y = torch.linspace(y1 / img.shape[0] * 2 - 1, y2 / img.shape[0] * 2 - 1, h)\n",
    "            grid_y, grid_x = torch.meshgrid(y, x, indexing=\"ij\")\n",
    "            coords = torch.stack([grid_x, grid_y], dim=-1).reshape(h * w, 2)\n",
    "\n",
    "            # Obliczenie wartości t_value\n",
    "            t_value = int(os.path.splitext(frame_name)[0])\n",
    "            t_value = (t_value * 2) / (self.VIDEO_LENGTH - 1)\n",
    "            t = torch.full((coords.shape[0], 1), t_value, dtype=torch.float32)\n",
    "            coords = torch.cat([coords, t], dim=-1)\n",
    "\n",
    "            # Przekształcenie obrazu i maski do formatu tensorów\n",
    "            img = torch.tensor(img).view(-1, 3)\n",
    "            mask = torch.tensor(mask).view(-1, 1)\n",
    "\n",
    "            output = {\n",
    "                \"coordinates\": coords,  # (x, y, t)\n",
    "                \"original_image\": img,  # Obraz oryginalny\n",
    "                \"original_mask\": mask,  # Maska oryginalna\n",
    "                \"rectangle_coords\": [x1, y1, x2, y2],  # Współrzędne prostokąta\n",
    "            }\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471affd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    dir_name: Path, batch_size: int = 1, num_workers: int = 0\n",
    ") -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Tworzy obiekty DataLoader dla zbiorów treningowego i walidacyjnego.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_name : Path\n",
    "        Ścieżka do katalogu głównego zawierającego dane.\n",
    "    batch_size : int, optional\n",
    "        Rozmiar batcha dla loadera treningowego, domyślnie 1.\n",
    "    num_workers : int, optional\n",
    "        Liczba wątków używanych do ładowania danych, domyślnie 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[DataLoader, DataLoader]\n",
    "        Zwraca tuple zawierającą DataLoader dla zbioru treningowego, walidacyjnego.\n",
    "    \"\"\"\n",
    "    # Ścieżka do zbioru treningowego\n",
    "    train_path = Path(dir_name / \"train\")\n",
    "    frame_names = sorted(os.listdir(train_path / \"images\"))\n",
    "    train_dataset = ImageDataset(\n",
    "        Path(train_path / \"images\"),\n",
    "        Path(train_path / \"masks\"),\n",
    "        frame_names,\n",
    "        mode=\"train\",\n",
    "    )\n",
    "\n",
    "    # Ścieżka do zbioru walidacyjnego\n",
    "    val_path = Path(dir_name / \"val\")\n",
    "    frame_names = sorted(os.listdir(val_path / \"images\"))\n",
    "    val_dataset = ImageDataset(\n",
    "        Path(val_path / \"images\"),\n",
    "        Path(val_path / \"masks\"),\n",
    "        frame_names,\n",
    "        mode=\"val\",\n",
    "        json_path=val_path / \"rectangles.json\",\n",
    "    )\n",
    "\n",
    "    # Tworzenie DataLoaderów\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=1, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "tempdir = tempfile.TemporaryDirectory()\n",
    "TMP_DIR = tempdir.name\n",
    "DATA_PATH = Path(TMP_DIR) / Path(\"data\")\n",
    "\n",
    "def unpack_tar_gz(filename: str, path: Path = DATA_PATH) -> None:\n",
    "    \"\"\" Rozpakowuje archiwum tar.gz \"\"\"\n",
    "    with tarfile.open(filename, \"r:gz\") as tar:\n",
    "        tar.extractall(path=path)\n",
    "    \n",
    "unpack_tar_gz(\"./train.tar.gz\", DATA_PATH / Path(\"train\"))\n",
    "unpack_tar_gz(\"./val.tar.gz\", DATA_PATH / Path(\"val\"))\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    DATA_PATH, batch_size=2, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb27d0",
   "metadata": {},
   "source": [
    "### Kod z Kryterium Oceniającym\n",
    "\n",
    "Kod, zbliżony do poniższego, będzie używany do oceny rozwiązania na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "\n",
    "def grade(\n",
    "    model: torch.nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device = \"cuda\",\n",
    ") -> tuple[float, float, float]:\n",
    "    \"\"\"Ocena modelu na podstawie danych z DataLoadera.\n",
    "\n",
    "    Funkcja oblicza średni PSNR (Peak Signal-to-Noise Ratio) oraz dokładność (accuracy)\n",
    "    dla masek, a także zwraca końcową punktację.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        Model do oceny.\n",
    "    data_loader : torch.utils.data.DataLoader\n",
    "        DataLoader zawierający dane do oceny.\n",
    "    device : torch.device\n",
    "        Urządzenie, na którym wykonywane są obliczenia (np. 'cuda' lub 'cpu').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, float, float]\n",
    "        Zwraca tuple zawierającą:\n",
    "        - Punkty końcowe (float)\n",
    "        - Średni PSNR (float)\n",
    "        - Średnią dokładność (float)\n",
    "    \"\"\"\n",
    "    model.eval()  # Ustawienie modelu w tryb ewaluacji\n",
    "    psnr_list, acc_list = [], []  # Listy do przechowywania wyników PSNR i dokładności\n",
    "\n",
    "    with torch.no_grad():  # Wyłączenie gradientów dla ewaluacji\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            # Pobranie danych z batcha\n",
    "            coordinates = batch[\"coordinates\"].to(device)\n",
    "            original_image = (\n",
    "                batch[\"original_image\"][0].cpu().numpy().reshape(256, 256, 3)\n",
    "            )\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            original_mask = batch[\"original_mask\"][0].cpu().numpy().reshape(256, 256)\n",
    "            rect_coords = batch[\"rectangle_coords\"]  # (x1, y1, x2, y2)\n",
    "\n",
    "            # Przesłanie modelu na GPU\n",
    "            model = model.cuda()\n",
    "            rgb_pred, mask_pred = model(coordinates)\n",
    "            rgb_pred = rgb_pred.squeeze(0).detach().cpu().numpy()\n",
    "            rgb_pred = rgb_pred[:, [2, 1, 0]]\n",
    "            mask_pred = mask_pred.squeeze(0).detach().cpu().numpy().squeeze()\n",
    "\n",
    "            # Tworzenie obrazów zrekonstruowanych\n",
    "            inpainted_img = original_image.copy()\n",
    "            inpainted_mask = original_mask.copy()\n",
    "            x1, y1, x2, y2 = map(int, rect_coords)\n",
    "            idx = 0\n",
    "            for y in range(y1, y2):\n",
    "                for x in range(x1, x2):\n",
    "                    inpainted_img[y, x, :] = rgb_pred[idx]\n",
    "                    inpainted_mask[y, x] = mask_pred[idx]\n",
    "                    idx += 1\n",
    "\n",
    "            # Obliczanie PSNR dla regionu prostokąta\n",
    "            gt_region = original_image[y1:y2, x1:x2, :]\n",
    "            pred_region = inpainted_img[y1:y2, x1:x2, :]\n",
    "            psnr = peak_signal_noise_ratio(gt_region, pred_region, data_range=1.0)\n",
    "            psnr_list.append(psnr)\n",
    "\n",
    "            # Obliczanie dokładności dla regionu maski\n",
    "            gt_mask_region = original_mask[y1:y2, x1:x2]\n",
    "            pred_mask_region = inpainted_mask[y1:y2, x1:x2]\n",
    "            pred_mask_region = (pred_mask_region > 0.5).astype(np.uint8)\n",
    "            acc = np.mean(pred_mask_region == gt_mask_region)\n",
    "            acc_list.append(acc)\n",
    "\n",
    "            # Wizualizacja wyników dla pierwszego batcha\n",
    "            if batch_idx == 0:\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                axs[0].imshow(original_image)\n",
    "                axs[0].set_title(\"Original Image\")\n",
    "                axs[1].imshow(inpainted_img)\n",
    "                axs[1].set_title(\"Inpainted Image\")\n",
    "                axs[2].imshow(inpainted_mask, cmap=\"gray\")\n",
    "                axs[2].set_title(\"Inpainted Mask\")\n",
    "                for ax in axs:\n",
    "                    ax.axis(\"off\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "    # Obliczanie średnich wyników PSNR i dokładności\n",
    "    psnr_score = np.mean(psnr_list)\n",
    "    acc_score = np.mean(acc_list)\n",
    "\n",
    "    # Obliczanie punktacji na podstawie PSNR\n",
    "    if psnr_score < 15.5:\n",
    "        p_psnr = 0\n",
    "    elif psnr_score < 23.5:\n",
    "        p_psnr = (psnr_score - 15.5) * 5/4\n",
    "    else:\n",
    "        p_psnr = 10\n",
    "\n",
    "    # Obliczanie punktacji na podstawie dokładności\n",
    "    if acc_score < 0.83:\n",
    "        p_acc = 0\n",
    "    elif acc_score < 0.98:\n",
    "        p_acc = (acc_score - 0.83) * 20/3\n",
    "    else:\n",
    "        p_acc = 1\n",
    "\n",
    "    # Łączna punktacja\n",
    "    points = 7 * p_psnr + 30 * p_acc\n",
    "\n",
    "    return int(round(points, 0)), psnr_score, acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44fb15",
   "metadata": {},
   "source": [
    "### Przykładowe Rozwiązanie\n",
    "\n",
    "Poniżej przedstawiamy uproszczone rozwiązanie, które służy jako przykład demonstrujący podstawową funkcjonalność notatnika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "class DummyINR(nn.Module):\n",
    "    \"\"\"\n",
    "    DummyINR to prosty model sieci neuronowej, który generuje różowe wartości RGB\n",
    "    oraz maskę zerową na podstawie podanych współrzędnych wejściowych.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Inicjalizuje instancję klasy DummyINR.\"\"\"\n",
    "        super(DummyINR, self).__init__()\n",
    "\n",
    "    def forward(self, coords):\n",
    "        \"\"\"\n",
    "        Oblicza różowe wartości RGB oraz maskę zerową na podstawie współrzędnych wejściowych.\n",
    "\n",
    "        Parametry:\n",
    "        ----------\n",
    "        coords : torch.Tensor\n",
    "            Tensor wejściowy o kształcie (B, N, 3), gdzie B to rozmiar batcha,\n",
    "            N to liczba punktów, a 3 to współrzędne (x, y, t).\n",
    "\n",
    "        Zwraca:\n",
    "        -------\n",
    "        tuple:\n",
    "            Dwie wartości zawierające:\n",
    "                - rgb : torch.Tensor\n",
    "                    Tensor różowych wartości RGB w zakresie [0, 1] o kształcie (B, N, 3).\n",
    "                - mask : torch.Tensor\n",
    "                    Tensor maski zerowej o kształcie (B, N, 1).\n",
    "        \"\"\"\n",
    "        batch_size, num_points, _ = coords.shape\n",
    "\n",
    "        pink_rgb = torch.tensor(\n",
    "            [1, 0, 1], dtype=torch.float32, device=coords.device\n",
    "        )\n",
    "        rgb = pink_rgb.unsqueeze(0).unsqueeze(0).expand(batch_size, num_points, -1)  # Shape: (B, N, 3)\n",
    "\n",
    "        # Mask values in the range [0, 1]\n",
    "        mask = torch.zeros(\n",
    "            (batch_size, num_points, 1), dtype=torch.float32, device=coords.device\n",
    "        )  # Shape: (B, N, 1)\n",
    "\n",
    "        return rgb, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a079af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    dummy_model = DummyINR()\n",
    "    points, psnr, accuracy = grade(dummy_model, val_loader)\n",
    "    print(f\"Przykładowe rozwiązanie uzyskało: {points} pkt. na zbiorze walidacyjnym.\")\n",
    "    print(f\"PSNR: {psnr:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fad452",
   "metadata": {},
   "source": [
    "## Twoje Rozwiązanie\n",
    "\n",
    "W tej sekcji należy umieścić Twoje rozwiązanie. Wprowadzaj zmiany wyłącznie tutaj!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourSolution(nn.Module):\n",
    "    \"\"\"Klasa YourSolution\n",
    "\n",
    "    Klasa implementuje model sieci neuronowej INR, który przetwarza współrzędne\n",
    "    wejściowe (x, y, t) i zwraca wartości RGB oraz maskę.\n",
    "\n",
    "    Atrybuty:\n",
    "    ----------\n",
    "    Brak atrybutów do zainicjalizowania w konstruktorze.\n",
    "    Model należy zdefiniować w metodzie `__init__`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Inicjalizuje model YourSolution.\n",
    "\n",
    "        Tutaj należy zdefiniować wszystkie warstwy i komponenty modelu.\n",
    "        \"\"\"\n",
    "        super(YourSolution, self).__init__()\n",
    "        # Inicjalizacja modelu\n",
    "        pass\n",
    "\n",
    "    def forward(self, coords):\n",
    "        \"\"\"Przetwarza współrzędne wejściowe i zwraca wyniki modelu.\n",
    "\n",
    "        Parametry:\n",
    "        ----------\n",
    "        coords : torch.Tensor\n",
    "            Tensor wejściowy o kształcie (B, N, 3), gdzie każda kolumna\n",
    "            odpowiada współrzędnym (x, y, t).\n",
    "\n",
    "        Zwraca:\n",
    "        --------\n",
    "        tuple\n",
    "            Dwie wartości wyjściowe:\n",
    "                - \"rgb\" : torch.Tensor\n",
    "                    Tensor z wartościami RGB w zakresie [0, 1].\n",
    "                - \"mask\" : torch.Tensor\n",
    "                    Tensor z wartościami maski z zakresu [0, 1].\n",
    "        \"\"\"\n",
    "        # Implementacja modelu\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    epochs=1,\n",
    "    lr=1000,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    \"\"\"Funkcja trenująca model na podanym zbiorze danych.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        Model, który ma zostać wytrenowany.\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        DataLoader zawierający dane treningowe.\n",
    "    epochs : int, optional\n",
    "        Liczba epok treningowych (domyślnie 1).\n",
    "    lr : float, optional\n",
    "        Współczynnik uczenia (domyślnie 1000).\n",
    "    device : str, optional\n",
    "        Urządzenie, na którym ma być trenowany model (domyślnie \"cuda\", jeśli dostępne).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Funkcja wymaga dalszej implementacji pętli treningowej oraz logiki uczenia.\n",
    "    \"\"\"\n",
    "    # Zbuduj pętlę uczącą\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=lr\n",
    "    )  # Możesz zmienić optymalizator\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podaj liczbę epok, które chcesz przeprowadzić i wybrany współczynnik uczenia\n",
    "EPOCHS = 40\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec61e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "model = YourSolution()\n",
    "train(model, train_loader, epochs=EPOCHS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a113dc",
   "metadata": {},
   "source": [
    "## Ewaluacja\n",
    "\n",
    "Uruchomienie poniższej komórki pozwoli sprawdzić, ile punktów zdobyłoby Twoje rozwiązanie na danych walidacyjnych. Przed wysłaniem upewnij się, że cały notebook wykonuje się od początku do końca bez błędów w trybie *FINAL_EVALUATION_MODE = True* i bez konieczności ingerencji użytkownika po wybraniu opcji \"Run All\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    points, psnr, accuracy = grade(model, val_loader)\n",
    "    print(f\"Twoje rozwiązanie uzyskało: {points} pkt. na zbiorze walidacyjnym.\")\n",
    "    print(f\"PSNR: {psnr:.2f}\")\n",
    "    print(f\"Dokładność klasyfikacji: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68fc336",
   "metadata": {},
   "source": [
    "Podczas sprawdzania model zostanie zapisany jako `your_model.pkl` i oceniony na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if FINAL_EVALUATION_MODE:\n",
    "    import cloudpickle\n",
    "\n",
    "    OUTPUT_PATH = \"file_output\"\n",
    "    FUNCTION_FILENAME = \"your_model.pkl\"\n",
    "    FUNCTION_OUTPUT_PATH = os.path.join(OUTPUT_PATH, FUNCTION_FILENAME)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "    with open(FUNCTION_OUTPUT_PATH, \"wb\") as f:\n",
    "        cloudpickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
